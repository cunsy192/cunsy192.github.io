<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>村山羊的博客</title>
  
  <subtitle>追随属于你的星星</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://cunsy192.github.io/"/>
  <updated>2022-04-18T14:26:39.574Z</updated>
  <id>https://cunsy192.github.io/</id>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>卷积神经网络（手写体-MNIST数据集）的PyTorch实现</title>
    <link href="https://cunsy192.github.io/2021/10/03/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84PyTorch%E5%AE%9E%E7%8E%B0/"/>
    <id>https://cunsy192.github.io/2021/10/03/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84PyTorch%E5%AE%9E%E7%8E%B0/</id>
    <published>2021-10-03T13:22:00.000Z</published>
    <updated>2022-04-18T14:26:39.574Z</updated>
    
    <content type="html"><![CDATA[<p>抽空实现了下计算机视觉中最基本的“Hello World”，Mnist数据集。<br>    <a id="more"></a><br>这里我用的是Jupyter Notebook，它比较方便快捷，适合进行快速debug，当然缺点就是自建库的调用不太方便。我将在GitHub上传该项目，欢迎star:)</p><p>一、数据集处理<br>我用的是从百度网盘下载的（应该没有侵权吧233）的Mnist数据集，差不多长这个样子：</p><p><img src="/images/pasted-77.png" alt="upload successful"><br>对，这是一种特殊的压缩格式，网络上现在很多都是直接用Pytorch下载，而我认为其实很多情况下都需要处理一些奇怪的数据类型，理想的“即插即用”的情况实在太少了。可见数据处理是门学问（只对我而言哈哈）。<br>这个格式可以用np.load()函数装载，读入后会以列表的形式装着四个子文件夹，分别为测试集与训练集的图像数据和标签，那么我们就可以都用numpy来表示这些数据了；之后，为了让这些数据能加速，需要转化成张量（Tensor）放入GPU，于是转化成张量的torch.from_numpy()也是必不可少的。这部分代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#加载数据集与预处理</span></span><br><span class="line">data = np.load(<span class="string">'mnist.npz'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#读入数据（numpy格式）</span></span><br><span class="line">x_train = data[<span class="string">'x_train'</span>]</span><br><span class="line">y_train = data[<span class="string">'y_train'</span>]</span><br><span class="line">x_test = data[<span class="string">'x_test'</span>]</span><br><span class="line">y_test = data[<span class="string">'y_test'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#转化为tensor</span></span><br><span class="line">x_train = torch.from_numpy(x_train)</span><br><span class="line">y_train = torch.from_numpy(y_train)</span><br><span class="line">x_test = torch.from_numpy(x_test)</span><br><span class="line">y_test = torch.from_numpy(y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment">#数据图像增加通道（numpy转化为tensor无通道）</span></span><br><span class="line">x_train = torch.unsqueeze(x_train, dim=<span class="number">1</span>)</span><br><span class="line">x_test = torch.unsqueeze(x_test, dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>这里有个图像增加通道，是因为我发现读入的数据只有三维（训练集是60000×28×28，600000是batch size，表示有600000张图片堆在一起处理），而后续处理需要增加一个channel，变成（60000×1×28×28）。<br>之后，要把数据和标签一一对应起来，这需要用到Torch中的Dataset类方法，我们需要继承这个父类并编写<strong>getitem</strong>()、<strong>len</strong>()、<strong>add</strong>()方法（这个方法可以不写）和初始化<strong>init</strong>()。我们希望这个类能够把x和y封装在一起，便于后边操作。我这里是这么写的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#数据封装，方便之后网络处理</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mnist_Dataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data, label)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.data = data</span><br><span class="line">        self.label = label</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        data = self.data[index]</span><br><span class="line">        labels = self.label[index]</span><br><span class="line">        <span class="keyword">return</span> data, labels</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.data)</span><br><span class="line"></span><br><span class="line">train_data = Mnist_Dataset(x_train, y_train)</span><br><span class="line">test_data = Mnist_Dataset(x_test, y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将数据放入数据加载器</span></span><br><span class="line">train_dataloader = DataLoader(train_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>后面调用只要用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_data = Mnist_Dataset(x_train, y_train)</span><br><span class="line">test_data = Mnist_Dataset(x_test, y_test)</span><br></pre></td></tr></table></figure><p>来封装数据即可。事实上这里我也只是依样画葫芦，具体的原理需要查阅其他文献，不过我发现大量的Dataset重构中，<strong>init</strong>()基本都是用来传入路径的，而<strong>getitem</strong>()主要用来对图像进行transform（如reshape操作等）并返回图像及label。<br>最后，我们需要一个DataLoader来装载Dataset,这个DataLoader是一个迭代器，将Dataset划分为若干个Batch，并打乱数据集（如果shuffle=True的话），在后续训练时，每次取出迭代器中的一个Batch进行训练，若后续利用优化器进行梯度下降，则实现了小批量梯度下降。当然，如果你的Batch设成1或者设成数据集的大小的话，也就实现了随机梯度下降或小批量梯度下降。<br>代码看这里：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将数据放入数据加载器</span></span><br><span class="line">train_dataloader = DataLoader(train_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>二、网络及其定义<br>查阅相关论文，找到LetNet这一经典网络，我们继承nn.Module类，书写LetNet这一类：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#定义LeNet网络</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(LeNet, self).__init__()</span><br><span class="line">        self.conv1 = nn.Sequential(     <span class="comment">#input_size=(1*28*28)</span></span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>), </span><br><span class="line">            nn.ReLU(),      <span class="comment">#input_size=(6*28*28)</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),<span class="comment">#output_size=(6*14*14)</span></span><br><span class="line">        )</span><br><span class="line">        self.conv2 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">3</span>),</span><br><span class="line">            nn.ReLU(),      <span class="comment">#input_size=(16*10*10)</span></span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)  <span class="comment">#output_size=(16*5*5)</span></span><br><span class="line">        )</span><br><span class="line">        self.fc1 = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">16</span> * <span class="number">6</span> * <span class="number">6</span>, <span class="number">120</span>),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line">        self.fc2 = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义前向传播过程，输入为x</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        <span class="comment"># nn.Linear()的输入输出都是维度为一的值，所以要把多维度的tensor展平成一维</span></span><br><span class="line">        x = x.view(x.size()[<span class="number">0</span>], <span class="number">-1</span>)</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>可以看出，LetNet由俩函数组成，第一个是集成父类和定义网络结构的<strong>init</strong>函数，而另一个定义了前向传播，这里面关于nn.Conv2d等是Torch库的写法，不再赘述。<br>在使用网络时，我们需要传入一个四维张量（batch×channel×长×宽），这个网络自动输出10类手写体（0~9）每一类的概率。这里是代码，其中inputs是(64×1×28×28)的张量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">outputs = model(inputs)</span><br></pre></td></tr></table></figure><p>三、训练<br>我们已经定义了数据集与网络，现在首先要做的是定义损失函数与优化器，这里选的损失函数是交叉熵损失，因为交叉熵损失不用将标签转化为one-hot类型，而优化器用的是经典的Adam。需要注意的是，我们定义了一个model，这个model就是LeNet()，但是为了让它能在GPU上加速，要让它放在cuda上（没有的话就cpu咯）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#指定使用的具体设备</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line">model = LeNet().to(device)</span><br><span class="line">optimizer = optim.Adam(model.parameters())</span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure><p>最后，我们开始训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(total_epoch):</span><br><span class="line">    epoch_loss = <span class="number">0.0</span></span><br><span class="line">    train_correct = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        inputs, labels = data</span><br><span class="line"></span><br><span class="line">        inputs, labels = Variable(inputs).cuda(), Variable(labels).cuda()</span><br><span class="line">        inputs = inputs.float()</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        l = loss(outputs, labels)</span><br><span class="line">        l.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        _,id = torch.max(outputs.data, <span class="number">1</span>)</span><br><span class="line">        epoch_loss += l.data</span><br><span class="line">        train_correct += torch.sum(id == labels.data)</span><br><span class="line">        </span><br><span class="line">    <span class="comment">#打印出epoch、每个epoch的总loss以及精度</span></span><br><span class="line">    print(<span class="string">'epoch:%d | epoch_loss:%.03f | train_correct: %.03f'</span>%(epoch+<span class="number">1</span>,epoch_loss,train_correct/len(train_data)))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"training finish!"</span>)</span><br></pre></td></tr></table></figure><p>这里其实有点问题，每个epoch的loss是每个batch loss的平均值，这里用的是累积值。<br>Variable(inputs).cuda(), Variable(labels).cuda()用于将inputs和labels放到GPU上，没有这个会显示Tensor的类型不一致而报错，inputs = inputs.float()也是为了让数据一致，你可以试试去掉会有什么事情发生。<br>对了，反向传播前要将优化器梯度清零，所以有optimizer.zero_grad()；而optimizer.step()是为了更新l的值。<br>为了判断正确率，这里设置输出最大的outputs的值作为最终的分类结果，并和label进行对比，正确则train_correct加一，然后就是老生常谈的输出了，这块输出是这样的：</p><p><img src="/images/pasted-78.png" alt="upload successful"></p><p>训练完后保存模型，在测试时加载模型的代码长这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#保存训练模型</span></span><br><span class="line">torch.save(model, <span class="string">'LeNet'</span>)</span><br><span class="line"><span class="comment">#加载训练模型</span></span><br><span class="line">model = torch.load(<span class="string">'LeNet'</span>)</span><br></pre></td></tr></table></figure><p>四、测试<br>这里和训练基本没区别，只是要导入测试集、没有epoch以及没有反向传播而已，祭出代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#对测试集进行测试</span></span><br><span class="line">test_loss = <span class="number">0.0</span></span><br><span class="line">test_correct = <span class="number">0</span></span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> test_dataloader:</span><br><span class="line">    inputs, labels = data</span><br><span class="line"></span><br><span class="line">    inputs, labels = Variable(inputs).cuda(), Variable(labels).cuda()</span><br><span class="line">    inputs = inputs.float()</span><br><span class="line">        </span><br><span class="line">    outputs = model(inputs)</span><br><span class="line">    l = loss(outputs, labels)</span><br><span class="line">        </span><br><span class="line">    _,id = torch.max(outputs.data, <span class="number">1</span>)</span><br><span class="line">    test_loss += l.data</span><br><span class="line">    test_correct += torch.sum(id == labels.data)</span><br><span class="line">        </span><br><span class="line">print(<span class="string">'loss:%.03f | train_correct: %.03f'</span>%(test_loss,test_correct/len(test_data)))</span><br></pre></td></tr></table></figure><p>最终准确率为98.7%，loss为9.380，效果还是很好的。</p><p>五、总结<br>其实感觉没啥好总结的，但是做出手写体识别还是挺有成就感的，之后大概会更新一些其他模型吧，哈哈。那么今天就到这里了，再见大家。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;抽空实现了下计算机视觉中最基本的“Hello World”，Mnist数据集。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="我与AI" scheme="https://cunsy192.github.io/categories/%E6%88%91%E4%B8%8EAI/"/>
    
    
      <category term="计算机视觉" scheme="https://cunsy192.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>使用BP神经网络实现二分类</title>
    <link href="https://cunsy192.github.io/2020/10/23/%E4%BD%BF%E7%94%A8BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%E4%BA%8C%E5%88%86%E7%B1%BB/"/>
    <id>https://cunsy192.github.io/2020/10/23/%E4%BD%BF%E7%94%A8BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%E4%BA%8C%E5%88%86%E7%B1%BB/</id>
    <published>2020-10-23T10:18:42.000Z</published>
    <updated>2020-10-23T13:16:34.584Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/pasted-70.png" alt="upload successful"></p><pre><code>神经网络类型：BP(back propagation)神经网络隐藏层：1（含神经元：3，偏置项独立，梯度下降关闭）激活函数：sigmoid函数正则化：无迭代次数：1000数据集：内外两包含一定厚度的以10为边界的1/4圆噪点：30个，两边各15个编程语言：Python</code></pre><p>  没想到吧，我又回来了！这两周光准备期中考试了，都没有时间来学习课外知识，但想到不发博客对不起自己的良心，还是拿之前做过的染色分类器水水文章。<br>    <a id="more"></a><br>对了，在文章开始前我想介绍一位朋友，这位不愿透露性别的朋友要我帮忙宣传ta的博客，于是你可以看见如下链接：<br><a href="https://www.cnblogs.com/Desc-End/" target="_blank" rel="noopener">Desc_End的博客</a><br>好了，广告时间结束，进入正题！<br>在本文开始之前，你需要了解：<br>一、线性代数基础<br>二、Python基础（含numpy、matplotlib.pyplot、random库）<br>三、高等数学基础（主要含链式求导法则）<br>四、线性回归基础<br>五、深度学习基础（BP神经网络原理）<br><del>是不是感觉似曾相识</del><br>OK，Here we go!<br>一、线性激活函数与sigmoid函数<br>  上文中用到了y=x这一激活函数，输出成为了拟合的数值点，在这里，我们使用了sigmoid函数，将输出变成了“概率”，当然它不是真正意义上的概率[1]。在程序中，我使用了定义：<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(t)</span>:</span></span><br><span class="line">  x = <span class="number">1.0</span>/(<span class="number">1</span>+np.exp(-t))</span><br><span class="line">  <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><br>  需要注意的坑是，这里的t为向量，而且exp函数只能调用np库中的exp，使用math库会报错，在附录中，你[可以]下载到程序，我将代码做了一个伪UI界面，你可以愉快调参，输出各种不同的结果。<br>  <del>笔者玩得不亦乐乎，它每次迭代的过程都不同( •̀ ω •́ )y</del><br>二、数据集设定<br>  我采用的是内外圆设定，x0、x1构成横轴与纵轴，y用于标记数据，大概就是这样（？）：</p><p><img src="/images/pasted-71.png" alt="upload successful"></p><pre><code>这里的数据域是离散的点，在效果图（见第一幅图）中，我在正类与负类中均选取了100个点。</code></pre><p>  我同时设置了<br>三、正向传播<br>  和之前一样，我采用了一层神经网络，这里不再赘述，只是将输出的神经元用sigmoid处理，调整阈值为0.5（和逻辑回归很像），单层神经网络节点模型如下：</p><p><img src="/images/pasted-72.png" alt="upload successful">  </p><p>  多个节点就组成了神经网络：</p><p><img src="/images/pasted-73.png" alt="upload successful"> </p><p>  最后得到的是0-1之间的数，大于等于0.5就是正类，小于0.5就是负类。<br>四、反向传播<br>  没啥好说的，之前写过了，这里提醒两点：<br>  1、由于使用了sigmoid函数，导致求导后变成了z(1-z)这种形式（详见吴恩达）；<br>  2、建议关闭对于偏置项的梯度下降，这样能防止过拟合（挂一个过拟合图像）：</p><p><img src="/images/pasted-74.png" alt="upload successful"></p><pre><code>虽然是100%正确率，但是完全没有价值（除了好看）</code></pre><p>五、测试新数据<br>  学习完后我们需要看看效果如何，于是我引入了新数据测试点，在标题界面展示所属点的类型（红点/蓝点），结果还是很精确的（BP神经网络如此强大！）<br>六、结尾<br>  是的，<del>本文就是把很久以前的代码翻出来然后恬不知耻地水了篇博客</del>本文对最简单的BP染色分类器做了实现，不算太好，但起码能看。最后安利一波VsCode，打开速度比pycharm好很多，还有很多非常舒服的功能，感谢泡哥！<br>七、附录<br>  1、参考文献<br>  [1]<a href="https://blog.csdn.net/euzmin/article/details/104538328" target="_blank" rel="noopener">sigmoid输出的是什么</a><br>  2、下载项<br>  [1]<a href="/download/BP_Classfication.py" ">BP染色分类器</a></p><p>  下一个模型是什么呢？</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/pasted-70.png&quot; alt=&quot;upload successful&quot;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;神经网络类型：BP(back propagation)神经网络
隐藏层：1（含神经元：3，偏置项独立，梯度下降关闭）
激活函数：sigmoid函数
正则化：无
迭代次数：1000
数据集：内外两包含一定厚度的以10为边界的1/4圆
噪点：30个，两边各15个
编程语言：Python&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;  没想到吧，我又回来了！这两周光准备期中考试了，都没有时间来学习课外知识，但想到不发博客对不起自己的良心，还是拿之前做过的染色分类器水水文章。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>一个简单的神经网络实例</title>
    <link href="https://cunsy192.github.io/2020/10/07/%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E4%BE%8B/"/>
    <id>https://cunsy192.github.io/2020/10/07/%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E4%BE%8B/</id>
    <published>2020-10-07T04:13:00.000Z</published>
    <updated>2020-10-23T12:42:31.114Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/pasted-67.png" alt="upload successful"></p><pre><code>神经网络类型：BP(back propagation)神经网络隐藏层：1（含神经元：3，偏置项独立）激活函数：y=x正则化：无迭代次数：300拟合函数：y=sin(x/3)数据集：本地随机生成编程语言：Python</code></pre><p>那天舍友找到我，问我搞不搞BP神经网络，于是我就自闭了两天。现在，一个清晰的图像呈现在我眼前，它是拟合众多实例中的一个。</p><a id="more"></a><p>注意：本文不对BP的原理做出详尽的解释，具体推导过程请参阅[1]，这里只对模型进行编程化表示，想要代码的直接翻到文末，因为过程主要是我对机器学习课程的理解，似乎有点冗长，并存在未知的问题，但代码可以运行,有需要的小伙伴可以下载。<br>在本文开始之前，你需要了解：<br>一、线性代数基础<br>二、Python基础（含numpy、matplotlib.pyplot、random库）<br>三、高等数学基础（主要含链式求导法则）<br>四、线性回归基础<br>五、深度学习基础（BP神经网络原理）</p><p>OK，我们开始快乐建模与coding！<br>以下述函数为例：$$y={x^2}$$<br>一、模型的建立与表示<br>   我选择了最简单的神经网络模型，即一个输入层，一个隐藏层，一个输出层，激活函数[2]选择最简单的y=x，因为我们要拟合函数。如图：</p><p><img src="/images/pasted-68.png" alt="upload successful"></p><p>偏置项与之前的回归模型不同，这里独立出来，之后会提到。<br>二、数据初始化、矩阵化与正向传播<br>   这里采用了连续间隔为1的x离散图，x∈[-10,10]，设<br>   $$y={x^2+rd}$$<br>   其中rd为-5/3到5/3的随机数，用于制造噪点。<br>   假设数据有n列（这里n为21），矩阵表示$X^0$，有：</p><p>   $$<br>   X^0=<br>  \begin{pmatrix}<br>  x_{1} \\<br>  x_{2} \\<br>  \vdots \\<br>  x_{n} \\<br>  \end{pmatrix}<br>   $$</p><pre><code>MathJax我打不出来同时带上下标的公式QAQ</code></pre><p>   相同地，$Y$表示为：<br>   $$<br>   Y=<br>  \begin{pmatrix}<br>  y_{1} \\<br>  y_{2} \\<br>  \vdots \\<br>  y_{n} \\<br>  \end{pmatrix}<br>   $$<br>   权重阵${θ_1}$为：<br>   $$<br>   \begin{pmatrix}<br>   θ_1^1 &amp; θ_2^1 &amp; θ_3^1<br>   \end{pmatrix}<br>   $$</p><p>   权重阵${θ_2}$为：<br>   $$<br>   \begin{pmatrix}<br>   θ_1^2 \\<br>   θ_2^2 \\<br>   θ_3^2 \\<br>   \end{pmatrix}<br>   $$</p><pre><code>公式有点问题，只能用上标表示行，以下标表示列，向量要以该形式表示，没有问题，这里加个猛男嘻嘻~</code></pre><p>   两个偏置项b1，b2，分别用两个列向量表示（初值置为1）：<br>   $$<br>   \begin{pmatrix}<br>   b_1^1 \\<br>   b_2^1 \\<br>   \vdots \\<br>   b_n^1 \\<br>   \end{pmatrix}<br>   \begin{pmatrix}<br>   b_1^2 \\<br>   b_2^2 \\<br>   \vdots \\<br>   b_n^2 \\<br>   \end{pmatrix}<br>   $$</p><p>   值得注意的是，该处xθ为矩阵相乘，最后是一个3×3的矩阵：</p><p>   $$<br>   \begin{equation}<br>   \begin{split}<br>   xθ_1+b1= \\<br>   \end{split}<br>   \end{equation}<br>   $$</p><p>   $$<br>   \begin{pmatrix}<br>   x_{1}^1 &amp; x_{1}^2 &amp; x_{1}^3 \\<br>   x_{2}^1 &amp; x_{2}^2 &amp; x_{2}^3 \\<br>   \vdots &amp; \vdots &amp; \vdots \\<br>   x_{n}^1 &amp; x_{n}^2 &amp; x_{n}^3 \\<br>   \end{pmatrix}<br>   $$</p><p>   这个矩阵很有意思，他的每一个列向量表示一个神经元，我们的所有节点均为这样的列向量，如果你按我的思路推导，就能得出以上结论。<br>   继续输出（正向传播），得到output层：</p><p>   $$<br>   \begin{equation}<br>   \begin{split}<br>   xθ_2+b2=<br>   \end{split}<br>   \end{equation}<br>   $$</p><p>   $$<br>   \begin{pmatrix}<br>   y_1^{‘} \\<br>   y_2^{‘} \\<br>   \vdots \\<br>   y_n^{‘} \\<br>   \end{pmatrix}<br>   $$</p><p>   至此，正向传播构建完毕<del>我快给公式搞疯了</del><br>三、误差与梯度<br>   根据吴恩达课程所教内容，每层误差为：<br>   $$<br>   \begin{equation}<br>   \begin{split}<br>   δ^{2}=output-Y \qquad (1)<br>   \end{split}<br>   \end{equation}<br>   $$<br>   $$<br>   \begin{equation}<br>   \begin{split}<br>   δ^{1}=δ^{2}θ^T \qquad (2)<br>   \end{split}<br>   \end{equation}<br>   $$</p><pre><code>真的，这公式有点丑，但我才刚刚上手MathJax，不用在意~</code></pre><p>   这里的（2）与吴恩达不同，因为我们使用了y=x作激活函数，导数就为1，而吴恩达的为sigmoid激活函数，所以有所不同。<br>   好了，到求梯度环节！<br>   根据链式求导法则[3]，有</p><p>   $$<br>   Δ^{(2)}:=\frac{1}{m}(Δ^{(2)}+{layer1}^Tδ^{2}) \\<br>   Δ^{(1)}:=Δ^{(1)}+x^Tδ^{1}<br>   $$</p><p>   $Δ^{(1)}$与$Δ^{(2)}$即为每层连接矩阵的梯度向量，这里的$Δ^{(k)},k=1,2$刚开始时置为0，我在编程中使用了笨方法，即用两行代码表示两个梯度项，没有如吴恩达他老人家展开成一列向量，我这么做的坏处就是不易更新与维护，whatever，我会继续更新算法的。<br>四、梯度下降与循环<br>   在这里，我使用了最传统的梯度下降法，没有使用高级算法（造业余轮子一百年），有：<br>   $$<br>   θ_1:=θ_1-aΔ^{(1)} \\<br>   θ_2:=θ_2-aΔ^{(2)}<br>   $$<br>   其中a为学习率，取一&lt;1的数，我一般取0.1、0.3、0.003等等，这是为了防止梯度爆炸[4]，不瞒你说，我在实际运行中出现过很多次梯度爆炸。<br>   大功告成！但是等等，我们似乎漏掉了偏置项！根据链式求导，我们<del>轻松地</del>求出了偏置项的梯度下降公式为：<br>   $$<br>   b1=b1-aδ^{1}<br>   b2=b2-aδ^{1}<br>   $$<br>   切记！这里的梯度下降是直接对误差δ求的，不是对Δ！<br>   之后，我们做一个循环，这与回归类型一致，不再赘述。<br>五、运行结果<br>   先上一张运行效果图！（btw，我加入了误差和多图表示，使得程序与上述有些不同，但你应该能分辨吧hh）</p><p><img src="/images/pasted-69.png" alt="upload successful">   </p><pre><code>左边的图为散点拟合图，右边为随迭代次数升高的误差，可以看到误差在降低。</code></pre><p>   有些拟合函数的误差并没有这么完美，它们是先降低后升高再降低的，就像这个：</p><p>   <a href="/download/一般三次函数.png" ">点击查看一般三次函数图</a></p><p>   除此之外，还有许多有趣的图片，你可以点击下方链接来查看它们：</p><p>   <a href="/download/sigmoid_function.png" ">点击查看拟合sigmoid图</a><br>   <a href="/download/简单正弦函数.png" ">点击查看拟合简单正弦函数图</a><br>   <a href="/download/过原点反比例函数.png" ">点击查看拟合过原点反比例函数图</a><br>   <a href="/download/一般一次函数.png" ">点击查看拟合一般一次函数图</a></p><p>   你可以在下载项的源代码中关掉动画项，如果你的计算机算力不强的话（具体请见源代码注解）。<br>   好了，今天就到这吧，我们分类问题见！<br>六、参考资料与下载项<br>  1、参考资料</p><p>  [1]周志华.机器学习[M].清华大学出版社:周志华.2016-1-1<br>  [2]<a href="https://wenku.baidu.com/view/2a88edc8e53a580216fcfeda.html" target="_blank" rel="noopener">激活函数</a><br>  [3]<a href="https://baike.baidu.com/item/%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99/3314017?fr=aladdin" target="_blank" rel="noopener">链式求导法则</a><br>  [4]<a href="https://blog.csdn.net/junjun150013652/article/details/81274958" target="_blank" rel="noopener">梯度消失与梯度爆炸</a></p><p>  2、下载项</p><p>  <a href="/download/BP_fitting.py" ">BP神经网络拟合程序</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/pasted-67.png&quot; alt=&quot;upload successful&quot;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;神经网络类型：BP(back propagation)神经网络
隐藏层：1（含神经元：3，偏置项独立）
激活函数：y=x
正则化：无
迭代次数：300
拟合函数：y=sin(x/3)
数据集：本地随机生成
编程语言：Python&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;那天舍友找到我，问我搞不搞BP神经网络，于是我就自闭了两天。现在，一个清晰的图像呈现在我眼前，它是拟合众多实例中的一个。&lt;/p&gt;
    
    </summary>
    
    
      <category term="我与AI" scheme="https://cunsy192.github.io/categories/%E6%88%91%E4%B8%8EAI/"/>
    
    
      <category term="机器学习" scheme="https://cunsy192.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://cunsy192.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>公式测试</title>
    <link href="https://cunsy192.github.io/2020/09/30/%E5%85%AC%E5%BC%8F%E6%B5%8B%E8%AF%95/"/>
    <id>https://cunsy192.github.io/2020/09/30/%E5%85%AC%E5%BC%8F%E6%B5%8B%E8%AF%95/</id>
    <published>2020-09-30T03:53:52.000Z</published>
    <updated>2020-09-30T03:57:57.958Z</updated>
    
    <content type="html"><![CDATA[<p>$$<br>P = \frac<br>{\sum_{i=1}^n (x_i- x)(y_i- y)}<br>{\displaystyle \left[<br>\sum_{i=1}^n (x_i-x)^2<br>\sum_{i=1}^n (y_i-y)^2<br>\right]^{1/2} }<br>$$<br>竟然成功了，惊了个呆</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;$$&lt;br&gt;P = \frac&lt;br&gt;{\sum_{i=1}^n (x_i- x)(y_i- y)}&lt;br&gt;{\displaystyle \left[&lt;br&gt;\sum_{i=1}^n (x_i-x)^2&lt;br&gt;\sum_{i=1}^n (y_i-y)^2&lt;br&gt;\right
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>逻辑回归——多分类</title>
    <link href="https://cunsy192.github.io/2020/09/30/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E2%80%94%E2%80%94%E5%A4%9A%E5%88%86%E7%B1%BB/"/>
    <id>https://cunsy192.github.io/2020/09/30/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E2%80%94%E2%80%94%E5%A4%9A%E5%88%86%E7%B1%BB/</id>
    <published>2020-09-29T16:31:34.000Z</published>
    <updated>2020-09-30T02:44:25.125Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/pasted-65.png" alt="upload successful"></p><p>现在，你看到了一个多分类的效果图，它是怎么实现的呢？</p><a id="more"></a><p>一、基础准备<br>准备工作在开始本文前，你需要具备逻辑回归二分类应该具备的基础，该模型是建立在二分类的基础上，利用矩阵特性进行拓展得到的多分类模型。<br>二、矩阵改进<br>我在上一篇博客中已经讲述了二分类所需的构造矩阵，在这里，我将对矩阵进行改进。好的，首先是。。额，随机数生成。。这步只是建立一个本地数据库，便于调试，可以略去。<br><del>我再重申一遍，我没在开玩笑，会xlrd库的大佬举个手好吗！</del><br>而后到了重头戏！根据吴恩达机器学习[1]，我们每次将数据分为两类：正类和负类，体现在数据上是标注0和1，一共划分n次，这里取3次（例子啦，不要纠结~）。根据矩阵知识，重构后的矩阵如下：</p><p><img src="/images/pasted-66.png" alt="upload successful"></p><p>可以很明显地看出，多分类其实就是将因变量和权重矩阵扩充了，于是在之后的逻辑回归中只需迭代n次而不是3n次，减少了时间复杂度。我将这种运算模式称为：并行计算。理所当然地，因变量矩阵和权重矩阵构成了并行运算矩阵模块。<br>三、逻辑回归<br>这和之前的二分类问题没有区别，只是在运算sigmoid函数时需要小小改进一下：<br>首先，将误差阵A转变为一个由numpy库生成的空矩阵而取代列表转矩阵，这样我们只需确定空矩阵接受误差向量的维度（即θ列向量的行数，现在是θ矩阵！）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#某置空矩阵默默从多分类的全世界路过</span></span><br><span class="line">A = np.empty(shape=[<span class="number">0</span>,theta.shape[<span class="number">0</span>]])</span><br></pre></td></tr></table></figure><p>需要注意的是，在误差的梯度下降中，我们需要将A重新置空，并且不能把θ阵和损失阵的减序搞反。<del>康康是哪个笨比进行了上面的操作，然后成功的把阈值降低到了0.3。</del></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#就是这厮，减号后面的项与前面的项不能搞反，会导致阈值变化</span></span><br><span class="line">theta = theta - (<span class="number">1</span>/m) * alpha * A</span><br></pre></td></tr></table></figure><p>四、输出与绘图<br>枯燥无味的输出时间！仍然是函数泛化，只是我们需要一个迭代器将三条直线一次输出，我采用的是x轴不变，y轴循环更新的方法，其实之前是想用列表添加三个y域的，但是太麻烦<del>主要是根本不会列表添加列表与取值，甚至我打一个函数还要考虑半天它的输出（快哭了）</del>，采用此方法不会保留列表值，每次输出后自动损失y的数据。大功告成！<br>五、不足与改进措施<br>首先非常明显，该程序是针对本地数据集的，当然你可以调整本地数据集的范围，但仍然使得这个程序非常菜鸡。然后，你可以轻松地想到，它的学习率需要手动调整（调参侠！），迭代次数也不能保证，当然这是可以用损失函数优化的，这里不展示了。最后，它只能线性拟合标记的数据集，使得程序的作用范围远远达不到要求，这要求我们能够用更牛逼的模型计算问题，然后让大家交口称赞。<br>六、下载项<br><a href="/download/Multiple_Classification.py" ">多分类主程序</a><br><a href="/download/多分类.png" ">效果图</a><br>七、参考资料<br>[1]<a href="https://www.bilibili.com/video/BV164411b7dx?from=search&seid=14471657955032075192" target="_blank" rel="noopener">吴恩达机器学习（B站）</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/pasted-65.png&quot; alt=&quot;upload successful&quot;&gt;&lt;/p&gt;
&lt;p&gt;现在，你看到了一个多分类的效果图，它是怎么实现的呢？&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>一种简单的二分类模型</title>
    <link href="https://cunsy192.github.io/2020/09/29/%E4%B8%80%E7%A7%8D%E7%AE%80%E5%8D%95%E7%9A%84%E4%BA%8C%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/"/>
    <id>https://cunsy192.github.io/2020/09/29/%E4%B8%80%E7%A7%8D%E7%AE%80%E5%8D%95%E7%9A%84%E4%BA%8C%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/</id>
    <published>2020-09-28T16:45:00.000Z</published>
    <updated>2020-09-29T16:32:30.413Z</updated>
    
    <content type="html"><![CDATA[<p>本文将简单介绍逻辑回归中的常用模型——基于梯度下降优化的逻辑回归二分类模型的编程实践。<br>在本文开始前，你需要具备这些基础：<br>1、线性代数<br>2、导数与偏导数<br>3、概率论与数理统计基础（主要指逻辑斯蒂函数[1]的运用）<br>4、线性回归模型<br>5、逻辑回归模型的概念</p><a id="more"></a><p>好，我们开始吧。<br>一、数据设置与建立矩阵<br>该部分我使用预先设置好的数据集，便于观察效果。<br><del>大佬可以用xlrd库直接导入，由于我刚学Python太菜了没有办法。</del><br>以列表形式给出后，构造矩阵，这里将自变量、因变量与权重分别分为三个矩阵：</p><p><img src="/images/pasted-56.png" alt="upload successful"></p><p>二、构造梯度下降损失单次迭代<br>设自变量矩阵为X，因变量矩阵为Y，权重矩阵为θ，有：</p><p><img src="/images/pasted-61.png" alt="upload successful">       </p><p>（我下次有时间得把渲染公式的插件搞上去，插入图片真是累死人）<br>此步需要注意求相乘构成的列向量的矩阵须用np.exp()函数，用math.exp()会出大问题！</p><p>我们将真实的因变量矩阵表示为Y0，那么误差为：</p><p><img src="/images/pasted-58.png" alt="upload successful"></p><p>这里我们取Y作为前面的减数，去掉绝对值。<br>根据损失函数[2]，求导并经过矩阵变换得到：</p><p><img src="/images/pasted-60.png" alt="upload successful"></p><p>此步我们在迭代中设置一个空矩阵A用于储存迭代的误差与向量星乘之和a通过np.c_[]压入空矩阵A，再将A依次与列向量相乘并求和，最后按以上公式迭代（A须转置）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#伪代码</span></span><br><span class="line">a = sum((Y-Y0)*xi</span><br></pre></td></tr></table></figure><p>三、构造循环<br>此步同线性回归，设置循环结束条件iterations、步长（学习率）α、计数器cnt、数据个数m，套上第二步中所述梯度下降，完成。<br>四、函数泛化及输出<br>此步推导该公式使得三维决策平面降到二维决策边际：</p><p><img src="/images/pasted-62.png" alt="upload successful"></p><p>设置x1在数据的最小值与最大值之间（此处取0.1间距），输出。<br>五、效果展示</p><p><img src="/images/pasted-63.png" alt="upload successful"></p><pre><code>迭代次数：10000学习率：0.1无初值扰动可以看见，决策边际将圆点与三角点区分开来</code></pre><p>轨迹图如下：</p><p><img src="/images/pasted-64.png" alt="upload successful"></p><pre><code>迭代次数：100学习率：0.1无初值扰动可以看见，该决策边际从平行x轴出发，经过多次拟合，最后得到橘黄色直线（斜率最大负值的一条）</code></pre><p><del>老实说，有上下摆动我是没想到的</del></p><p>六、下载项</p><p><a href="/download/Binary_Classification.py" ">二分类逻辑回归</a></p><p><a href="/download/逻辑回归-二分类.png" ">效果图</a></p><p>七、参考资料<br>[1]<a href="https://baike.baidu.com/item/Logistic%E5%87%BD%E6%95%B0/3520384?fr=aladdin" target="_blank" rel="noopener">逻辑斯蒂函数</a><br>[2]<a href="https://www.zhihu.com/question/47744216?from=profile_question_card" target="_blank" rel="noopener">损失函数</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将简单介绍逻辑回归中的常用模型——基于梯度下降优化的逻辑回归二分类模型的编程实践。&lt;br&gt;在本文开始前，你需要具备这些基础：&lt;br&gt;1、线性代数&lt;br&gt;2、导数与偏导数&lt;br&gt;3、概率论与数理统计基础（主要指逻辑斯蒂函数[1]的运用）&lt;br&gt;4、线性回归模型&lt;br&gt;5、逻辑回归模型的概念&lt;/p&gt;
    
    </summary>
    
    
      <category term="我与AI" scheme="https://cunsy192.github.io/categories/%E6%88%91%E4%B8%8EAI/"/>
    
    
      <category term="机器学习" scheme="https://cunsy192.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Python" scheme="https://cunsy192.github.io/tags/Python/"/>
    
      <category term="矩阵" scheme="https://cunsy192.github.io/tags/%E7%9F%A9%E9%98%B5/"/>
    
  </entry>
  
  <entry>
    <title>由Python重构的线性回归算法</title>
    <link href="https://cunsy192.github.io/2020/09/27/%E7%94%B1Python%E9%87%8D%E6%9E%84%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/"/>
    <id>https://cunsy192.github.io/2020/09/27/%E7%94%B1Python%E9%87%8D%E6%9E%84%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/</id>
    <published>2020-09-26T18:12:00.000Z</published>
    <updated>2020-09-29T16:32:44.434Z</updated>
    
    <content type="html"><![CDATA[<p>本文将简要介绍将matlab中的线性回归算法迁移至Python中，即重构。因为今后的工作将越来越多地借助Python与其十分awesome的库，所以使用Python编程显得尤为重要。<br>本文将包括如下几个部分：<br>1、预备知识<br>2、线性回归算法<br>3、代码实现<br>4、情绪化的个人表达（可以略去）<br>5、参考文献及下载资料</p><a id="more"></a><p>1、预备知识<br>在进入线性回归的美妙时光之前，你需要掌握以下知识：<br>    Ⅰ.Python基本语法及numpy、matplotlib.pyplot、xlrd库。<br>    Ⅱ.线性代数基础<br>    Ⅲ.高等数学（主要是微分与导数）基础<br>    Ⅳ.熬夜精神与可能不充足但没有办法只能挤出来的时间<br>    Ⅴ.学长的鼓舞（这条选修）<br>2、线性回归算法<br>在该部分，我们将回顾主要的线性回归运算，但由于之前的博文已经写过，故略去。<br>3、代码实现<br>由于大部分与matlab中的程序相同，故主要选择不同之处与没有提及的地方讲解，关于之前的回归算法我在博文中有较为详细的说明，欢迎阅读。<br>    Ⅰ.导入数据<br>    导入数据需要用到xlrd库[1]，本文中不赘述（悄悄说句，matlab导入数据比Python好像更方便耶）<br>    Ⅱ.矩阵化<br>    由于matlab导入数据时可以选择矩阵，而Python莫得此功能，故我们需要苦逼地在本该是美好夜晚的时候让输入数据矩阵化（如果不这么做将导致后续处理数据时按列表处理，会产生求矩阵秩与乘法的问题，注意注意）。代码如下<br>    <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#矩阵化初始数据，须提前导入numpy库（这有时可用np.array代替，具体我没找到原因，留作思考题）</span></span><br><span class="line">np.matrix(x)</span><br></pre></td></tr></table></figure><br>    Ⅲ.转置矩阵<br>    这在matlab中十分方便，但是在这里，我们需要这个函数。<br>    <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#转置括号里那个可怜弱小又无助的矩阵QAQ</span></span><br><span class="line">np.transpose(x)</span><br></pre></td></tr></table></figure><br>    Ⅳ.求行列值、增扩矩阵、矩阵置零<br>    这几个功能在matlab中很好实现，但Python确实要费点脑子，主要是这几个函数。<br>    <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#进行求矩阵行列值的操作</span></span><br><span class="line">x.shape</span><br><span class="line"><span class="comment">#进行求矩阵行值或列值的操作（n取0为行值，n取1为列值）</span></span><br><span class="line">x.shape[n]</span><br><span class="line"><span class="comment">#生成零矩阵的函数[2]</span></span><br><span class="line">np.zero()</span><br><span class="line"><span class="comment">#矩阵增扩函数，y在左边则左增扩，右边则右增扩</span></span><br><span class="line">x = np.c_[x,y]</span><br></pre></td></tr></table></figure><br>    Ⅵ.矩阵运算<br>    还是matlab方便，但是我选择了Python，就要负责到底！加法和矩阵乘法和matlab类似，使用x*y或np.dot(x,y)函数，但我实在不明白为啥点乘就表示不出来，后来发现是要用这个函数[3]<br>    <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.multiply(a1, x1[:,i])</span><br></pre></td></tr></table></figure><br>    Ⅶ.作图<br>    你终于来到了这一步，但是摆在你眼前的是作图，没有关系，我们plt库可以当此重任！眼下的这几个函数与matlab中的这几个函数是等价的。<br>    <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#等价于scatter(x,y)</span></span><br><span class="line">plt.scatter(x,y)</span><br><span class="line"><span class="comment">#等价于plot(x,y)</span></span><br><span class="line">plt.plot(x,y)</span><br><span class="line"><span class="comment">#等价于hold on加上display</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><br>    Ⅷ.其他<br>    这可太棒了，我们解决了所有问题，但是等等，有一个注意点需要我们考虑。<br>    这个注意点在这里比较坑，我调了半天，从头开始才调出来，那就是减肥时，如果有两个列向量（列数一致），一个为矩阵，另一个为列表，则相减将得到意想不到的结果[4]，这就是之前所说的按列表处理的危害性。对了，来看看Python作的图！</p><p><img src="/images/pasted-52.png" alt="upload successful"></p><pre><code>散点图（初始数据集）</code></pre><p><img src="/images/pasted-53.png" alt="upload successful"></p><pre><code>迭代100次的收敛图像</code></pre><p><img src="/images/pasted-54.png" alt="upload successful"></p><pre><code>迭代轨迹图（从下往上，每条线代表每一次迭代产生的回归直线）</code></pre><p>4、情绪化的个人表达<br>其实像做这篇博文很久了，但因为懒和各种事情拖到现在，重构也是我花了3小时学了Python得出的结果（其实不能说学了，因为很多东西是现查的，而且有些东西需要摸索，网络上没有）。今天的重构是新的开始，之后的各类模型（除了数学建模）我都会使用Python写。人生苦短，我用Python！我今晚想起学长说的要猛学的话，决定要重新开始，不能再颓废了，于是我决定今晚一定要把重构代码搞出来，而且我做到了！有志者，事竟成！我决定在一周内发布逻辑回归的博客！<br>5、参考资料及下载文件<br>    ①参考资料<br>    [1]<a href="https://zhuanlan.zhihu.com/p/92678052" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/92678052</a><br>    [2]<a href="https://blog.csdn.net/cpc784221489/article/details/82885590" target="_blank" rel="noopener">https://blog.csdn.net/cpc784221489/article/details/82885590</a><br>    [3]<a href="https://blog.csdn.net/Cherry_Blossom_/article/details/79259804?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param" target="_blank" rel="noopener">https://blog.csdn.net/Cherry_Blossom_/article/details/79259804?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param</a><br>    [4]<a href="https://blog.csdn.net/dake13/article/details/80917932" target="_blank" rel="noopener">https://blog.csdn.net/dake13/article/details/80917932</a><br>    ②下载资料<br>    <a href="/download/LR.py" ">基于Python实现的线性回归</a><br>    <a href="/download/test.xlsx" ">Kaggle数据集（二元）</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将简要介绍将matlab中的线性回归算法迁移至Python中，即重构。因为今后的工作将越来越多地借助Python与其十分awesome的库，所以使用Python编程显得尤为重要。&lt;br&gt;本文将包括如下几个部分：&lt;br&gt;1、预备知识&lt;br&gt;2、线性回归算法&lt;br&gt;3、代码实现&lt;br&gt;4、情绪化的个人表达（可以略去）&lt;br&gt;5、参考文献及下载资料&lt;/p&gt;
    
    </summary>
    
    
      <category term="我与AI" scheme="https://cunsy192.github.io/categories/%E6%88%91%E4%B8%8EAI/"/>
    
    
      <category term="机器学习" scheme="https://cunsy192.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Python" scheme="https://cunsy192.github.io/tags/Python/"/>
    
      <category term="矩阵" scheme="https://cunsy192.github.io/tags/%E7%9F%A9%E9%98%B5/"/>
    
  </entry>
  
  <entry>
    <title>基于C语言单向循环链表的图书管理系统</title>
    <link href="https://cunsy192.github.io/2020/09/23/%E5%9F%BA%E4%BA%8EC%E8%AF%AD%E8%A8%80%E5%8D%95%E5%90%91%E5%BE%AA%E7%8E%AF%E9%93%BE%E8%A1%A8%E7%9A%84%E5%9B%BE%E4%B9%A6%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F/"/>
    <id>https://cunsy192.github.io/2020/09/23/%E5%9F%BA%E4%BA%8EC%E8%AF%AD%E8%A8%80%E5%8D%95%E5%90%91%E5%BE%AA%E7%8E%AF%E9%93%BE%E8%A1%A8%E7%9A%84%E5%9B%BE%E4%B9%A6%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F/</id>
    <published>2020-09-23T03:14:00.000Z</published>
    <updated>2020-09-26T19:30:47.453Z</updated>
    
    <content type="html"><![CDATA[<p>本文将简单阐述利用C语言单项循环链表所写的图书管理系统，并在文末附上代码。<br>一、系统可实现功能<br>  该系统由两部分子系统组成，分别为学生系统与管理系统。<br>  Ⅰ.学生系统<br>  该部分可以实现的功能有：查找图书、借阅图书、归还图书、显示图书表。<br>  Ⅱ.管理系统<br>  该部分可以实现的功能有：插入图书、删除图书、显示图书表、查找图书、统计信息。<br>  <a id="more"></a><br>二、功能实现<br>  1.主函数<br>  该部分主要用于调用各功能函数，并实现系统切换、分级与界面切换。主要包括：</p><pre><code>  ①欢迎界面该界面可选择退出、进入学生系统、进入管理系统，一旦调用系统，则进入第一级循环。②学生系统该界面由终止条件为0(int)的第二级子循环组成，下包括4个功能。    Ⅰ.查找图书功能    若进入该功能，输入1(int)，进入搜索循环（第三级子循环），下包括4个功     能：按编号查找、按书名查找、按作者查找、按借阅状态查找（0：已借出    1：可借阅），通过输入相应数字及字符串进行查找，并返回相应的查找内容，    按任意键退出后返回上一级。    Ⅱ.借阅图书功能    若进入该功能，输入2(int)，进而调用借阅函数，输入编号进行借阅。    Ⅲ.归还图书功能    若进入该功能，输入3(int)，进而调用归还函数，输入编号进行归还。    Ⅳ.显示当前图书表    若进入该功能，输入4(int)，进而调用打印函数，打印当前图书表，按任意键    返回上一级。③管理系统该界面由终止条件为0(int)的第二级子循环组成，下包括5个功能。    Ⅰ.插入图书功能    若进入该功能，则由终止条件为返回值为0(int)的循环插入函数组成，可以按    &apos;*&apos;号退出循环，否则继续插入。    Ⅱ.删除图书功能    若进入该功能，则调用删除界面函数，进而选择按编号、书名、作者删除，终    止条件为0(int)。    Ⅲ.显示当前图书表    同学生系统Ⅳ。    Ⅳ.查找图书功能    同学生系统Ⅰ。    Ⅴ.统计功能    若进入该功能，则统计图书信息及借阅情况，按任意键返回上一级。</code></pre><p>  2.各功能函数<br>  该部分为各功能函数介绍，函数简介如下：</p><pre><code><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">   <span class="comment">//返回值为0或1的欢迎界面函数</span></span><br><span class="line">   <span class="function"><span class="keyword">int</span> <span class="title">Home</span><span class="params">()</span></span>;</span><br><span class="line">   </span><br><span class="line">   <span class="comment">//返回值为0-4的学生界面函数</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Student</span><span class="params">()</span></span>;</span><br><span class="line">   </span><br><span class="line">   <span class="comment">//返回值为0-5的管理界面函数</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Admin</span><span class="params">()</span></span>;</span><br><span class="line">   </span><br><span class="line">   <span class="comment">//建立单项循环链表函数</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">InitdList</span><span class="params">(dLinklist *&amp;)</span></span>;</span><br><span class="line">   </span><br><span class="line">   <span class="comment">//返回值为0或1的插入功能函数</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Insert</span><span class="params">(dLinklist *)</span></span>;</span><br><span class="line">   </span><br><span class="line">   <span class="comment">//打印图书表函数</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Print</span><span class="params">(dLinklist *)</span></span>;</span><br><span class="line">   </span><br><span class="line">   <span class="comment">//返回值为0-4的搜索界面函数</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Search_Home</span><span class="params">()</span></span>; </span><br><span class="line">   </span><br><span class="line">   <span class="comment">//搜索功能函数</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Search</span><span class="params">(dLinklist *,<span class="keyword">int</span>)</span></span>;</span><br><span class="line">   </span><br><span class="line">   <span class="comment">//返回值为0-3的删除界面函数</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Delete_Home</span><span class="params">()</span></span>;</span><br><span class="line">   </span><br><span class="line">   <span class="comment">//删除功能函数</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Delete</span><span class="params">(dLinklist *,<span class="keyword">int</span>)</span></span>;</span><br><span class="line">   </span><br><span class="line">   <span class="comment">//统计功能函数</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Count</span><span class="params">(dLinklist *)</span></span>; </span><br><span class="line">   </span><br><span class="line">   <span class="comment">//借阅功能函数</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Borrow</span><span class="params">(dLinklist *)</span></span>;</span><br><span class="line">   </span><br><span class="line">   <span class="comment">//归还功能函数</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Return</span><span class="params">(dLinklist *)</span></span>;</span><br></pre></td></tr></table></figure></code></pre><p>  具体实现请下载附件。<br>三、功能概览<br>  如图所示。</p><p><img src="/images/pasted-46.png" alt="upload successful"></p><p><img src="/images/pasted-47.png" alt="upload successful"></p><p><img src="/images/pasted-48.png" alt="upload successful"></p><p><img src="/images/pasted-49.png" alt="upload successful"></p><p><img src="/images/pasted-50.png" alt="upload successful"></p><p><img src="/images/pasted-51.png" alt="upload successful"></p><p>四、附件下载<br>  附件中包含所有可运行函数，请使用DEV C++打开，使用VC等IDE须修改main返回值为void，并去掉主函数末尾返回值。</p><p><a href="/download/图书管理系统（正式版）v1.0.3.cpp" ">图书管理系统（正式版）v1.0.3</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将简单阐述利用C语言单项循环链表所写的图书管理系统，并在文末附上代码。&lt;br&gt;一、系统可实现功能&lt;br&gt;  该系统由两部分子系统组成，分别为学生系统与管理系统。&lt;br&gt;  Ⅰ.学生系统&lt;br&gt;  该部分可以实现的功能有：查找图书、借阅图书、归还图书、显示图书表。&lt;br&gt;  Ⅱ.管理系统&lt;br&gt;  该部分可以实现的功能有：插入图书、删除图书、显示图书表、查找图书、统计信息。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="项目" scheme="https://cunsy192.github.io/categories/%E9%A1%B9%E7%9B%AE/"/>
    
    
      <category term="C语言" scheme="https://cunsy192.github.io/tags/C%E8%AF%AD%E8%A8%80/"/>
    
  </entry>
  
  <entry>
    <title>基于matlab编程的“生日悖论”仿真模拟实验</title>
    <link href="https://cunsy192.github.io/2020/09/03/%E5%9F%BA%E4%BA%8Ematlab%E7%BC%96%E7%A8%8B%E7%9A%84%E2%80%9C%E7%94%9F%E6%97%A5%E6%82%96%E8%AE%BA%E2%80%9D%E4%BB%BF%E7%9C%9F%E6%A8%A1%E6%8B%9F%E5%AE%9E%E9%AA%8C/"/>
    <id>https://cunsy192.github.io/2020/09/03/%E5%9F%BA%E4%BA%8Ematlab%E7%BC%96%E7%A8%8B%E7%9A%84%E2%80%9C%E7%94%9F%E6%97%A5%E6%82%96%E8%AE%BA%E2%80%9D%E4%BB%BF%E7%9C%9F%E6%A8%A1%E6%8B%9F%E5%AE%9E%E9%AA%8C/</id>
    <published>2020-09-03T11:48:00.000Z</published>
    <updated>2020-09-27T10:18:31.073Z</updated>
    
    <content type="html"><![CDATA[<h3 align="center"> 摘要 </h3><p>  本文针对概率论老师提出的“生日悖论[1]”问题进行研究，通过matlab编程实现仿真，并将最终结果以图片形式表示出来。</p><a id="more"></a> <h5 align="left"> 关键词： </h5> 仿真 古典概型 生日悖论 matlab tabulate函数 函数化<h3 align="center"> 一、问题重述 </h3><p>  老师要求我们以“生日悖论”为题，在10人、30人、60人、90人的情况下随机输入每个人的生日y，输出图表，图表内容包括：<br>  1、横坐标为迭代次数x（x∈[1,200]）；<br>  2、纵坐标为日期y(y∈[1,365])；<br>  3、黑色叉号(x)表示不重复日期；<br>  4、红色叉号(red X)表示重复日期;<br>  5、标题包括本次迭代中仿真人数（/人）和至少有两人生日重复频率（P）。</p><h3 align="center"> 二、问题分析 </h3>  针对此问题，使用matlab编程，将1~365的随机数分别表示为10、30、60、90的向量，针对向量排序并统计重复数，将≥2的日期表示出来，并以向量表示，利用函数建立坐标，将每一次仿真的图像以黑色与红色叉号的形式表示在图上，重复200次，完成图像；同时，统计红色叉号与黑色叉号的数量，得出P；利用函数化主程序进行分块，得出结论。<h3 align="center"> 三、编程 </h3>首先解决随机化问题，这里使用了常见的随机数函数<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">round</span>(<span class="built_in">rand</span>(<span class="number">1</span>,h)*<span class="number">365</span>);</span><br></pre></td></tr></table></figure>利用plot打印黑叉。之后，利用tabulate()函数统计出向量各值的重复情况，这里以一次模拟为例：<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">...       ...      </span><br><span class="line"><span class="number">353</span>        <span class="number">0</span>      </span><br><span class="line"><span class="number">354</span>        <span class="number">0</span>      </span><br><span class="line"><span class="number">355</span>        <span class="number">2</span>      </span><br><span class="line"><span class="number">356</span>        <span class="number">0</span>      </span><br><span class="line"><span class="number">357</span>        <span class="number">0</span>     </span><br><span class="line"><span class="number">358</span>        <span class="number">0</span>     </span><br><span class="line"><span class="number">359</span>        <span class="number">3</span>    </span><br><span class="line"><span class="number">360</span>        <span class="number">0</span>    </span><br><span class="line"><span class="number">361</span>        <span class="number">0</span>     </span><br><span class="line"><span class="number">362</span>        <span class="number">0</span>      </span><br><span class="line"><span class="number">363</span>        <span class="number">0</span>    </span><br><span class="line"><span class="number">364</span>        <span class="number">1</span></span><br></pre></td></tr></table></figure>取第二列大于等于2所对应的日期，标记为另一个向量，并打印，标记为红叉；迭代200次，分块，得到最终图像，效果如下：<p><img src="/images/pasted-45.png" alt="upload successful"></p><p>可以看见，当人数大于30时，发生生日重复的概率已经超过70%，这与直觉相悖，故称“生日悖论”。</p><h3 align="center"> 四、总结 </h3><p>  本文针对生日悖论的实际情况进行仿真，得出以上图像，从而直观上证明了生日悖论的反直觉性。</p><h3 align="center"> 五、参考文献 </h3>  <p>[1]<a href="https://baike.baidu.com/item/%E7%94%9F%E6%97%A5%E6%82%96%E8%AE%BA/2715290?fr=aladdin" target="_blank" rel="noopener">https://baike.baidu.com/item/%E7%94%9F%E6%97%A5%E6%82%96%E8%AE%BA/2715290?fr=aladdin</a></p><h3 align="center"> 六、附件 </h3>  <p>函数：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[P]</span> = <span class="title">bir</span><span class="params">(h,it,x,y,z)</span></span></span><br><span class="line">cnt = <span class="number">0</span>;</span><br><span class="line">subplot(x,y,z);</span><br><span class="line"><span class="built_in">hold</span> on;</span><br><span class="line"><span class="keyword">for</span> k = <span class="number">1</span> : it,</span><br><span class="line">    A = <span class="built_in">round</span>(<span class="built_in">rand</span>(<span class="number">1</span>,h)*<span class="number">365</span>);</span><br><span class="line">    A  = <span class="built_in">sort</span>(A);</span><br><span class="line">    <span class="built_in">plot</span>(k,A,<span class="string">'black x'</span>);</span><br><span class="line">    <span class="built_in">hold</span> on;</span><br><span class="line">    M = tabulate(A);</span><br><span class="line">    X = [];</span><br><span class="line">    [m,n] = <span class="built_in">size</span>(M);</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : m,</span><br><span class="line">        <span class="keyword">if</span> M(<span class="built_in">i</span>,<span class="number">2</span>) &gt;= <span class="number">2</span>,</span><br><span class="line">        X = [X,M(<span class="built_in">i</span>,<span class="number">1</span>)];</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    aa = <span class="built_in">size</span>(X);</span><br><span class="line">    <span class="keyword">if</span> aa ~= <span class="number">0</span></span><br><span class="line">        <span class="built_in">plot</span>(k,X,<span class="string">'red X'</span>);</span><br><span class="line">        <span class="built_in">hold</span> on;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">if</span> aa == <span class="number">0</span></span><br><span class="line">        cnt = cnt + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">P = <span class="number">1</span> - cnt / it;</span><br><span class="line">title([num2str(h),<span class="string">'人，P='</span>,num2str(P)]);</span><br><span class="line"><span class="built_in">hold</span> on;</span><br></pre></td></tr></table></figure><p>主程序：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bir(<span class="number">10</span>,<span class="number">200</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>);</span><br><span class="line">bir(<span class="number">30</span>,<span class="number">200</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>);</span><br><span class="line">bir(<span class="number">60</span>,<span class="number">200</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>);</span><br><span class="line">bir(<span class="number">90</span>,<span class="number">200</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">4</span>);</span><br></pre></td></tr></table></figure><p>实验原图下载：<br><a href="/download/final.fig" ">final</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 align=&quot;center&quot;&gt; 摘要 &lt;/h3&gt;

&lt;p&gt;  本文针对概率论老师提出的“生日悖论[1]”问题进行研究，通过matlab编程实现仿真，并将最终结果以图片形式表示出来。&lt;/p&gt;
    
    </summary>
    
    
      <category term="数学建模" scheme="https://cunsy192.github.io/categories/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"/>
    
      <category term="作业随想录" scheme="https://cunsy192.github.io/categories/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/%E4%BD%9C%E4%B8%9A%E9%9A%8F%E6%83%B3%E5%BD%95/"/>
    
    
      <category term="matlab" scheme="https://cunsy192.github.io/tags/matlab/"/>
    
      <category term="概率论" scheme="https://cunsy192.github.io/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>数值计算相关方法（求根）</title>
    <link href="https://cunsy192.github.io/2020/06/28/%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97%E7%9B%B8%E5%85%B3%E6%96%B9%E6%B3%95%EF%BC%88%E5%88%86%E6%AD%A5%E6%9B%B4%E6%96%B0%EF%BC%89/"/>
    <id>https://cunsy192.github.io/2020/06/28/%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97%E7%9B%B8%E5%85%B3%E6%96%B9%E6%B3%95%EF%BC%88%E5%88%86%E6%AD%A5%E6%9B%B4%E6%96%B0%EF%BC%89/</id>
    <published>2020-06-28T14:43:00.000Z</published>
    <updated>2020-09-27T10:19:50.314Z</updated>
    
    <content type="html"><![CDATA[<h6 align="right"> 作者：村山羊 </h6>本文针对数值计算求根方法做出matlab的编程实现，并给出效果图和相对误差分析图（又可以水一篇博客了233）。<a id="more"></a>--------------------------<h1 align="center"> 目录 </h1><h3 align="left"> 序言 </h3><h3 align="left"> 读者须知 </h3><h4 align="left"> 1、交叉法求根 </h4><h6 align="left"> Ⅰ、逐步搜索法 </h6><h6 align="left"> Ⅱ、二分法 </h6><h6 align="left"> Ⅲ、比例求根法 </h6><h4 align="left"> 2、迭代法求根 </h4><h6 align="left"> Ⅰ、牛顿法 </h6><h6 align="left"> Ⅱ、弦截法 </h6><h3 align="left"> 参考文献 </h3><h3 align="left"> 附录 </h3><hr><h4 align="left"> 序言 </h4>    要求一个方程的根，这个经典的数学问题在初中就已经出现了，随着学历的增长，我们接触到了许多奇奇怪怪的方程，像n元m次方程等。我们求解这些方程的方式一般是利用数学的方式计算，但在工程数学中，很多时候我们不需要知道他们的确解，而是只要计算出他们的近似解就行，这种计算近似解的方法是数值计算要求掌握的。<h4 align="left"> 读者须知 </h4><p>(1)本文中所提到的观点纯属自己理解，有适当参考文献，并在文中标出引用，在文本参考文献提到；<br>(2)本文中提到的相对误差指的是|迭代当前值-收敛值/收敛值|；<br>(3)本文所著权归@村山羊，转载请联系QQ：1036814872，如有不足，欢迎提出；<br>(4)文中所用matlab为正版，所用程序可在matlab及相似环境中运行，全程序可供下载，具体移步文末附录；<br>(5)如有其他问题（如法务等），请及时联系本文作者，联系方式如上。<br>一个有解的方程有他的实根，但很多情况下我们解不出来（想起了高中的隐函数问题了吗hh）。这里以cosx-x=0和x<sup>12</sup>-1=0这两个方程为例。</p><h5 align="left"> 1、交叉法求根 </h5>交叉法其实在英文中为Bracketing Method，顾名思义，就是用括号去“括”根所在的区间，这里介绍两种方法。<h6 align="left"> Ⅰ、逐步搜索法 </h6>设有连续可导函数f(x)，存在区间x∈[a,b]，使得f(x)=0，x称为f(x)=0在[a,b]上的根。那么我们将a到b之间分为n等分，每一份求出其x对应的f(x)的值，根据零点存在定理[1]可以确定f(x)符号相反的相邻两点间存在方程的解，所以我们继续将这两个点作为新的区间，再重复上述步骤，直到得到最后的结果。例：编程求cosx-x=0在x∈[0,1]间的解。这里在区间中插入4个值，用matlab编程，代码如下：<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%设定端点值 a,b</span></span><br><span class="line"><span class="comment">%设定迭代次数 i</span></span><br><span class="line"><span class="comment">%设定步长（间隔距离）h</span></span><br><span class="line"><span class="comment">%设定函数值 fc</span></span><br><span class="line"><span class="comment">%设定空数组 k ,用于储存搜索区间内各值（迭代后清空）</span></span><br><span class="line">a = <span class="number">0</span>;</span><br><span class="line">b = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">i</span> = <span class="number">0</span>; </span><br><span class="line">h = (b-a)/<span class="number">5</span>;</span><br><span class="line"></span><br><span class="line">c = a; <span class="comment">%结果及计算过程用c表示</span></span><br><span class="line"></span><br><span class="line">fc = <span class="built_in">cos</span>(c)-c;</span><br><span class="line"></span><br><span class="line">k = [];</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="built_in">i</span> &lt; <span class="number">20</span>, <span class="comment">%设定迭代20次，也可用det误差设定条件</span></span><br><span class="line">    <span class="built_in">i</span> = <span class="built_in">i</span> + <span class="number">1</span>; <span class="comment">%迭代次数+1</span></span><br><span class="line">    <span class="comment">%求出搜索区间中各值</span></span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">0</span>:<span class="number">5</span>,</span><br><span class="line">        c = a + h*<span class="built_in">j</span>;</span><br><span class="line">        fc = <span class="built_in">cos</span>(c)-c;</span><br><span class="line">        k = [k,fc];</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> j1 = <span class="number">1</span>:<span class="number">5</span>,</span><br><span class="line">        <span class="keyword">if</span> k(j1)*k(j1+<span class="number">1</span>) &lt; <span class="number">0</span>,</span><br><span class="line">            b = a + h*j1;</span><br><span class="line">            a = a + h*(j1<span class="number">-1</span>);</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    h = (b-a)/<span class="number">5</span>;</span><br><span class="line">    k = [];</span><br><span class="line">    det(<span class="built_in">i</span>) = <span class="built_in">abs</span>(<span class="number">0.739085133215171</span> - c)/<span class="number">0.739085133215171</span>; <span class="comment">%相对误差</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>     需要注意的是，matlab中相对误差是估算出来的，便于计算迭代效率。<p>最终结果为</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">c =</span><br><span class="line"></span><br><span class="line">   <span class="number">0.739085133215171</span></span><br></pre></td></tr></table></figure><p>迭代次数-相对损失折线图如下：<br><img src="/images/pasted-33.png" alt="upload successful"><br>结果非常amazing啊，这个程序不到6次就近乎收敛，现在我们求另一个方程x<sup>12</sup>-1=0的正根。<br>例：编程求x<sup>12</sup>-1=0的正根。<br>相同地，我们对上面地程序进行改动，即将</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fc = <span class="built_in">cos</span>(c)-c;</span><br><span class="line">det(<span class="built_in">i</span>) = <span class="built_in">abs</span>(<span class="number">0.739085133215171</span> - c)/<span class="number">0.739085133215171</span>;</span><br></pre></td></tr></table></figure><p>改成</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fc = c^<span class="number">12</span><span class="number">-1</span>;</span><br><span class="line">det(<span class="built_in">i</span>) = <span class="built_in">abs</span>(<span class="number">1</span> - c)/<span class="number">1</span>;</span><br></pre></td></tr></table></figure><p>运行，刚好得到收敛的值</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">c =</span><br><span class="line"></span><br><span class="line">   <span class="number">1.000000000000053</span></span><br></pre></td></tr></table></figure><pre><code>这里的误差是计算机产生的，无法避免</code></pre><p>同样，迭代次数-相对损失折线图如下：</p><p><img src="/images/pasted-34.png" alt="upload successful"></p><p>也是不到几次就近乎收敛。</p><h6 align="left"> Ⅱ、二分法 </h6>现在来介绍二分法的相关内容。二分法这种方法其实在初等数学的学习中就已经掌握了，大学无非是对其做一个编程实现，重提下原理：设一连续可导函数f(x)，其f(x)=0的根存在[a,b]中，那么我们先将a到b的这块区间对半分((a+b)/2)，将其设为c，根据之前提到的零点定理求出根所在的区间，改变区域的值（是不是和之前的做法很像？），从而进一步缩小范围，直到足够逼近解。例：编程求cosx-x=0在x∈[0,1]间的解。<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%设定端点值 a,b</span></span><br><span class="line"><span class="comment">%设定迭代次数 i</span></span><br><span class="line"><span class="comment">%设定步长（间隔距离）h</span></span><br><span class="line"><span class="comment">%设定函数值 fc</span></span><br><span class="line"><span class="comment">%设定空数组 k ,用于储存搜索区间内各值（迭代后清空）</span></span><br><span class="line">a = <span class="number">0</span>;</span><br><span class="line">b = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">i</span> = <span class="number">0</span>; </span><br><span class="line"></span><br><span class="line">c = (a+b)/<span class="number">2</span>; </span><br><span class="line">fa = <span class="built_in">cos</span>(a)-a;</span><br><span class="line">fb = <span class="built_in">cos</span>(b)-b;</span><br><span class="line">fc = <span class="built_in">cos</span>(c)-c;</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="built_in">i</span> &lt; <span class="number">20</span>, <span class="comment">%设定迭代20次，也可用det误差设定条件</span></span><br><span class="line">    <span class="built_in">i</span> = <span class="built_in">i</span> + <span class="number">1</span>; <span class="comment">%迭代次数+1</span></span><br><span class="line">    <span class="keyword">if</span> fa*fc &lt; <span class="number">0</span>,</span><br><span class="line">        b = c;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">if</span> fb*fc &lt; <span class="number">0</span>,</span><br><span class="line">        a = c;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    c = (a+b)/<span class="number">2</span>;</span><br><span class="line">    fa = <span class="built_in">cos</span>(a)-a;</span><br><span class="line">    fb = <span class="built_in">cos</span>(b)-b;</span><br><span class="line">    fc = <span class="built_in">cos</span>(c)-c;</span><br><span class="line">    det(<span class="built_in">i</span>) = <span class="built_in">abs</span>(<span class="number">0.739085133216</span> - c)/<span class="number">0.739085133216</span>; <span class="comment">%相对误差</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>输出为：<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">c =</span><br><span class="line"></span><br><span class="line">   <span class="number">0.739084720611572</span></span><br></pre></td></tr></table></figure>误差图为：<p><img src="/images/pasted-35.png" alt="upload successful"><br>另一实例同理，这些程序将在文末的附录中给出。<br>例：编程求x<sup>12</sup>-1=0的正根。<br>程序见附录①，结果为</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">c =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span></span><br></pre></td></tr></table></figure><p>误差图如下：</p><p><img src="/images/pasted-37.png" alt="upload successful"></p><pre><code>一次即收敛！这是因为我用区间[0,2]来使用二分法，必然一次收敛。</code></pre><h6 align="left"> Ⅲ、比例求根法 </h6>经过前面两个方法，其实大家对交叉法的三个子方法应该有所了解了，这个比例求根法（据我所见）其实是对逐步搜索法和二分法的一个优化，将搜索的效率提高了。原理与二分法相同，只是不是真正地“二分”，而是按比例分割，这里仍以cosx-x=0及x<sup>12</sup>-1=0为例，代码见附录②及附录③，误差图如下：<h6 align="center"> cosx-x=0 </h6><p><img src="/images/pasted-38.png" alt="upload successful"></p><h6 align="center"> x<sup>12</sup>-1=0 </h6><p><img src="/images/pasted-39.png" alt="upload successful"></p><p>值得注意的是，这里的比例求根似乎不能发挥他的功能，迭代了越4000次才收敛，由于没有论证比例求根法的原理，故不做说明。      </p><p>由此，交叉法求根完结。</p><h4 align="left"> 2、迭代法求根 </h4>除了使用缩小区间的方式外，是否有其他方法求根？答案是肯定的，这就是迭代法。迭代法的思想在牛顿的那个时代早已提出，思想大概是这样的：先在要求根的附近取一点，然后根据这一点或相近两点的“陡峭程度”确定根的方向，并相对应地移向那里（是不是感觉和梯度下降很像？）。这里介绍两种方法。<h6 align="left"> Ⅰ、牛顿法 </h6>牛顿法的原理与泰勒展开式有关[2]，这里不再赘述，我们主要阐述其公式含义：<p><img src="/images/pasted-40.png" alt="upload successful">    </p><p>这个式子由选定点处的切线引出，与x轴相交后选定该点作为新点，之后的x由前一个x更新，直到f’(x)（选定点的切线斜率）趋于0后，右式后一项趋于无穷大，左式趋于0，精度达到要求后，迭代完毕。可能有点难理解，这里有找y=x<sup>2</sup>零点（x=0）的部分迭代图：</p><p><img src="/images/pasted-41.png" alt="upload successful"></p><pre><code>步骤一：选定点(5,25)，做切线与x轴交于x=2.5处。 </code></pre><p><img src="/images/pasted-42.png" alt="upload successful"></p><pre><code>步骤二：选定点(2.5,6.25)，做切线与x轴交于下一个点处，以此循环，直到斜率趋向0。</code></pre><p>这里的例子与之后的方法合起来进行对比，同样的，你可以在附录③与附录④中找到代码。</p><h6 align="left"> Ⅱ、弦截法 </h6>弦截法其实和牛顿法十分相像，只是弦截法是利用两个点做出一条直线与x轴相交，而牛顿法只使用一个点的切线作为直线，故选取点时要使用两个点（这一点老师在上课时与百度百科[3]中说的不太一样，百科中认为两个点围成的区间必须包括根，而课上所展示的PPT没有此意，故程序中均选取了十分相近的两点），弦截法迭代公式如下：<p><img src="/images/pasted-43.png" alt="upload successful"></p><pre><code>应该注意到，我们必须迭代三个值而不是牛顿法中的两个</code></pre><p>下相对误差图用于比较牛顿法与弦截法：</p><p><img src="/images/pasted-44.png" alt="upload successful"></p><h3 align="left"> 参考文献 </h3><p>[1]同济大学数学系.高等数学[M].北京:高等教育出版社,2014:68-69.</p><p>[2]Pikachu5808.牛顿法和拟牛顿法[DB/OL].<a href="https://zhuanlan.zhihu.com/p/46536960,2020" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/46536960,2020</a></p><p>[3]弦截法_百度百科[DB/OL].<a href="https://baike.baidu.com/item/%E5%BC%A6%E6%88%AA%E6%B3%95/1195626?fr=aladdin" target="_blank" rel="noopener">https://baike.baidu.com/item/%E5%BC%A6%E6%88%AA%E6%B3%95/1195626?fr=aladdin</a></p><h3 align="left"> 附录 </h3><p><a href="/download/bisection.m" ">附录①：二分法实例-x<sup>12</sup>-1=0</a></p><p><a href="/download/proportion.m" ">附录②：比例求根法实例-cosx-x=0</a></p><p><a href="/download/proportion1.m" ">附录③：比例求根法实例-x<sup>12</sup>-1=0</a></p><p><a href="/download/Newton_Raphson.m" ">附录④：牛顿法求根</a></p><p><a href="/download/Secant.m" ">附录⑤：弦截法求根</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h6 align=&quot;right&quot;&gt; 作者：村山羊 &lt;/h6&gt;
本文针对数值计算求根方法做出matlab的编程实现，并给出效果图和相对误差分析图（又可以水一篇博客了233）。
    
    </summary>
    
    
      <category term="数学建模" scheme="https://cunsy192.github.io/categories/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"/>
    
    
      <category term="matlab" scheme="https://cunsy192.github.io/tags/matlab/"/>
    
  </entry>
  
  <entry>
    <title>数模和高考这档子事</title>
    <link href="https://cunsy192.github.io/2020/06/08/%E6%95%B0%E6%A8%A1%E8%BF%99%E6%A1%A3%E5%AD%90%E4%BA%8B/"/>
    <id>https://cunsy192.github.io/2020/06/08/%E6%95%B0%E6%A8%A1%E8%BF%99%E6%A1%A3%E5%AD%90%E4%BA%8B/</id>
    <published>2020-06-08T08:50:00.000Z</published>
    <updated>2020-09-26T19:29:17.928Z</updated>
    
    <content type="html"><![CDATA[<p>下午2点睡的觉，再睁开眼已经4点。</p><a id="more"></a><p>队友群里传来消息，说是成绩出了，看了下校二等奖。<br>不是很牛逼的成绩，但是第一次做还是有些成就感的。<br>不过成就感这话和泡哥说可能更加合适，这是一个可敬的男人：3天rush建模coding论文一条龙，5月3日拿到5000字论文的我和队友的手微微颤抖。</p><pre><code>燊樣の侽仁：泡桑</code></pre><p>结果已定，想起那时思考论文结构和通宵（主要是补作业，线代我死了哭哭）的感觉，还是有种恍惚感。<br>说起来，现在是6月8日下午5点，去年这个时候，英语差不多考完了吧，我记得那天出来买了瓶芬达喝，现在想来有点酸。<br>为什么酸呢？说不上来。<br>之后的饭局、KTV和报考，仿佛昨日乍现，当真是：</p><pre><code>一切有为法,如梦幻泡影。如露亦如电,应作如是观。</code></pre><p>应作如是观，几人能做到？<br>俯仰之间，已为陈迹，<br>而兰亭已矣，梓泽丘墟，<br>今日抒怀，又是几番春秋？<br>遥想高考结束那时的夏天，<br>高山上的萤火虫，配着漫天星河，背靠漠白坟冢，生命、宏大、微小聚集于此，我拍下照片，似置身世外一般。</p><pre><code>无垠的星辰，微分了有限</code></pre><p>红警开源重置了，但是红警2的源码丢了，希望有生之年能看见红警2重置版吧。<br>我记得里面有个任务，叫做“好莱坞，梦一场”，名字真是妙。<br>果然，摄影能让人怀念造梦和怀梦的时光。<br>最后贴两张我拍的星空图，以怀念这高中的时光。</p><p><img src="/images/pasted-28.png" alt="upload successful"></p><p><img src="/images/pasted-31.png" alt="upload successful"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;下午2点睡的觉，再睁开眼已经4点。&lt;/p&gt;
    
    </summary>
    
    
      <category term="数学建模" scheme="https://cunsy192.github.io/categories/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"/>
    
      <category term="杂谈" scheme="https://cunsy192.github.io/categories/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/%E6%9D%82%E8%B0%88/"/>
    
    
  </entry>
  
  <entry>
    <title>装Tensorflow的辛酸史——安装&amp;杂谈</title>
    <link href="https://cunsy192.github.io/2020/06/03/%E8%A3%85Tensorflow%E7%9A%84%E8%BE%9B%E9%85%B8%E5%8F%B2/"/>
    <id>https://cunsy192.github.io/2020/06/03/%E8%A3%85Tensorflow%E7%9A%84%E8%BE%9B%E9%85%B8%E5%8F%B2/</id>
    <published>2020-06-03T14:34:00.000Z</published>
    <updated>2020-09-29T16:33:50.612Z</updated>
    
    <content type="html"><![CDATA[<p>我动笔写这篇文章的时候是6月3日晚上22：37分，在此之前的5个小时中，我一直处于受苦当中——当然不是像地狱一样无穷无尽的痛苦，亦不是雄鹰啄食普罗米修斯肝脏的苦痛——我最后还是装上了Tensorflow，除了结尾函数返回True的惊喜之外，是一种如见到古神般深深的不可名状的恐惧（愿长眠的克苏鲁候汝入眠，希望拉莱耶的宅邸光芒永存）。</p><a id="more"></a><p>这里是我san值没有清零前的琐碎话语：这次装Tensorflow本来想借助B站的力量，于是找到了莫烦python这位up来学习，然后看着看着就不由自主地翻到了评论，只见硕大的（这是夸张的说法，额，应该吧）url展现在眼前，下书由规范电脑字组成的一句话：“只要有手就能安装”。</p><p>好家伙，我现在想起来真得谢谢他。</p><p>于是我进入了该CSDN教程，在午觉后开始了惊心动魄的旅程。教程地址按老规矩放在文末①，以供查阅。</p><p>前面万事如意，不过在安装Tensorflow这一步出现了第一个错误，即无法使用ssl模块安装Tensorflow库，查了半天，大部分人员秉着热情好客的原则和对于ctrl+C和ctrl+v的迷恋与执着，告诉我要改ssl权限，后终于发现了正解②：在官网安装openssl（我现在严重怀疑是当时装python时没有选web功能导致的），然后安装，遂解。</p><p><img src="/images/pasted-26.png" alt="upload successful"></p><p>（贴张下载图来表示我激动的心情）</p><p>然后一路顺风，终于到了验证tensorflow能不能用的时刻，按命令，进入ipython，输入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure><p>豁，好家伙，这字体还会变色，大概就成了。不出意料地成功了。（小道消息：某知乎网友说这行代码在巴菲特看来值一个亿，当然这和让黄油和猫绑在一起做成永动机的真实姓差不多）</p><p>然后，在使用函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.test.is_gpu_available()</span><br></pre></td></tr></table></figure><p>时出了问题，主要表现为前几个信息都对，最后忽然给了个F，网上说是GPU的问题，又是添加路径又是重新下载驱动，最后发现了这篇文章③，说是要更新驱动。</p><p>于是我点开了nvidia的驱动界面，发现真的有更新可以使用。这时候，我犹豫了。</p><p>经历过ubuntu那与英伟达显卡不兼容的惨剧后，我只想好好说一句：”AMD,YES!”</p><p>不过想了一想自己的机会成本，还是按下了“快速安装”的按钮</p><p><button>快速安装</button></p><p>对，上面的这个按钮完全没有作用，他只是在我并不漫长的web前端中一个近乎失败的例子罢了（哭哭），我主要是为了表现一种快要成佛的心态。</p><p>就在这时，他报了错：“其他安装程序正在运行，安装失败”。</p><p>还好重启后恢复了正常，人生也像这样起起落落吧，习惯了。</p><p>最后安装完毕，我打开了cmd命令行，抱着必输的信念说到：“再不成功就不干了！”</p><p>在这时我按下了回车。</p><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">In</span> [<span class="number">2</span>]: tf.test.is_gpu_available()</span><br><span class="line">Successfully opened dynamic library cudart64_101.dll</span><br><span class="line">Successfully opened dynamic library cublas64_10.dll</span><br><span class="line">Successfully opened dynamic library cufft64_10.dll</span><br><span class="line">Successfully opened dynamic library curand64_10.dll</span><br><span class="line">Successfully opened dynamic library cusolver64_10.dll</span><br><span class="line">Successfully opened dynamic library cusparse64_10.dll</span><br><span class="line">Successfully opened dynamic library cudnn64_7.dll</span><br><span class="line">Adding visible gpu devices: <span class="number">0</span></span><br><span class="line">Device interconnect StreamExecutor with strength <span class="number">1</span> edge matrix:</span><br><span class="line">     <span class="number">0</span></span><br><span class="line"><span class="number">0</span>:   N</span><br><span class="line"><span class="function">GPU:0 <span class="title">with</span> 4702 <span class="title">MB</span> <span class="title">memory</span>) -&gt; <span class="title">physical</span> <span class="title">GPU</span></span></span><br></pre></td></tr></table></figure><p>最后，在我大脑开始搜索结果之前，出现了这么一句话，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Out[<span class="number">2</span>]: <span class="literal">True</span></span><br></pre></td></tr></table></figure><p>返回值为真。那一刻，除了欢呼，伴随着一种与世隔绝的恍惚感，如黄昏的光线找到了晨曦，如夜半的繁星看见了日出，如那一位学子心心念念的jvav，和清华的雷课堂。</p><p>Tensorflow库安装成功，接下来会使用其对之前所学和之后所学的机器学习内容进行python上的重写和改进。</p><p>文末链接：</p><p>①<a href="https://blog.csdn.net/weixin_44170512/article/details/103990592" target="_blank" rel="noopener">Tensorflow安装教程</a></p><p>②<a href="https://blog.csdn.net/m0_37872216/article/details/103121248?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase" target="_blank" rel="noopener">解决win10“ssl module in Python is not available”与国内镜像源加速</a></p><p>③<a href="https://www.cnblogs.com/ziyu-trip/p/12663978.html" target="_blank" rel="noopener">解决GPU问题（需操作后更新驱动）</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我动笔写这篇文章的时候是6月3日晚上22：37分，在此之前的5个小时中，我一直处于受苦当中——当然不是像地狱一样无穷无尽的痛苦，亦不是雄鹰啄食普罗米修斯肝脏的苦痛——我最后还是装上了Tensorflow，除了结尾函数返回True的惊喜之外，是一种如见到古神般深深的不可名状的恐惧（愿长眠的克苏鲁候汝入眠，希望拉莱耶的宅邸光芒永存）。&lt;/p&gt;
    
    </summary>
    
    
      <category term="杂谈" scheme="https://cunsy192.github.io/categories/%E6%9D%82%E8%B0%88/"/>
    
    
      <category term="机器学习" scheme="https://cunsy192.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="安装" scheme="https://cunsy192.github.io/tags/%E5%AE%89%E8%A3%85/"/>
    
  </entry>
  
  <entry>
    <title>逻辑回归——预告</title>
    <link href="https://cunsy192.github.io/2020/06/02/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>https://cunsy192.github.io/2020/06/02/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</id>
    <published>2020-06-02T15:05:00.000Z</published>
    <updated>2020-09-27T10:22:53.494Z</updated>
    
    <content type="html"><![CDATA[<p>在这篇文章中，我将分析逻辑回归的数学原理和数学模型，</p><a id="more"></a><p>并针对模型应用计算机语言编写相应的框架，最后对实例<br>进行分析。当所有的工作准备完成时，我将删除这段话，<br>以正式的博客内容取代。（不过真的有人看嘛hhh）</p><p>8.26更新<br>Wow，you can really gugu!<br>这篇文章不删了，我就是要向全世界展示鸽子的实力。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在这篇文章中，我将分析逻辑回归的数学原理和数学模型，&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://cunsy192.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>机器学习模型：线性回归（多元）</title>
    <link href="https://cunsy192.github.io/2020/05/31/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%EF%BC%9A%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%EF%BC%88%E5%A4%9A%E5%85%83%EF%BC%89/"/>
    <id>https://cunsy192.github.io/2020/05/31/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%EF%BC%9A%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%EF%BC%88%E5%A4%9A%E5%85%83%EF%BC%89/</id>
    <published>2020-05-31T04:11:00.000Z</published>
    <updated>2020-09-27T10:27:46.514Z</updated>
    
    <content type="html"><![CDATA[<p>在上篇文章中，我分析了一元线性回归的可行性与应用实例。</p><p>在这篇文章中，我将对一元线性回归的模型进行扩展，从而实现多元线性回归模型。</p><a id="more"></a><p>从matlab中线性回归核心代码：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:k,</span><br><span class="line">        A(<span class="built_in">i</span>) = (<span class="number">1</span>/m)*sum((x*theta-y).*x(:,<span class="built_in">i</span>));</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>看出，A的本质是一个行数与θ(j)个数相同，列数为1的向量，该向量与学习速率相乘后被相应的θ相减从而被迭代，可以预见：A的大小与x的大小无关，故可以扩展x的元数，形成多元的线性回归算法。</p><p>以下是模型要点：</p><p>①注意原始数据与矩阵数据的不同：因为多元函数有一项为常数，进行矩阵乘法时，需要对x(n×m)的左侧进行增扩一个n×1的列向量，形成的x1为n×(m+1)，如下图：</p><p><img src="/images/pasted-21.png" alt="upload successful"></p><pre><code>（原始数据集，导出为414×6的矩阵）</code></pre><p><img src="/images/pasted-22.png" alt="upload successful"></p><pre><code>（左侧增扩，形成414×7的矩阵）  </code></pre><p>②由于数据的不同性，学习速率必须做出相应的调整，以上的数据用学习速率为0.00000001经过迭代10000次后收敛，故将学习速率α（matlab中为a）单独做一个变量</p><p>③增扩矩阵与原始矩阵间必须有如①的对应关系，但是如果直接改变原始矩阵的列数，则下次重新运行程序时会将矩阵左侧再加一个列向量，导致运行失败，故一定要有一个矩阵（此处称为x1）继承并对应增扩矩阵。</p><p>④变量初始化问题：θ列向量在开始时应该设为0向量，以便后续操作，有语句：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">theta = <span class="built_in">zeros</span>(<span class="built_in">size</span>(x1,<span class="number">2</span>),<span class="number">1</span>);</span><br></pre></td></tr></table></figure><p>将θ设为0向量。</p><p>⑤迭代次数与学习速率：由于数据的特异性，需要对每一个具体分析的数据集调整不同的迭代次数（iterations）和学习速率(a)，测试后函数不再发散即可。</p><p>测试案例（数据来自kaggle）：<br>[Real estate](/download/datasets_88705_204267_Real estate.csv”)</p><p>工作区数据（运行后）：</p><p><img src="/images/pasted-23.png" alt="upload successful"></p><p>结果（a=0.00000001,iterations = 10000时）：</p><p><img src="/images/pasted-24.png" alt="upload successful"></p><p>测试样例正确性代码（取x1的某一行与θ相乘得到y1，看每一个y1结果是否与y近似）<br>：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> k = <span class="number">1</span>:<span class="number">414</span>,</span><br><span class="line">    y1(k) = x1(k,:)*theta;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:<span class="number">414</span>,</span><br><span class="line">    <span class="built_in">i</span>(<span class="built_in">j</span>) = <span class="built_in">j</span>;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">plot</span>(<span class="built_in">i</span>,y1,<span class="string">"."</span>);</span><br><span class="line"><span class="built_in">hold</span> on;</span><br><span class="line"><span class="built_in">plot</span>(<span class="built_in">i</span>,y',<span class="string">"."</span>);</span><br></pre></td></tr></table></figure><p>由此，得到散点图：</p><p><img src="/images/pasted-25.png" alt="upload successful"></p><p>（蓝色部分为拟合的y1，红色部分为原始数据y）<br>易见，拟合情况较好（不过由于该数据点分部较为分散，有较多y未能很好拟合）</p><p>最后，附上代码实例：</p><p><a href="/download/test1.m" ">多元线性回归模型代码</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在上篇文章中，我分析了一元线性回归的可行性与应用实例。&lt;/p&gt;
&lt;p&gt;在这篇文章中，我将对一元线性回归的模型进行扩展，从而实现多元线性回归模型。&lt;/p&gt;
    
    </summary>
    
    
      <category term="我与AI" scheme="https://cunsy192.github.io/categories/%E6%88%91%E4%B8%8EAI/"/>
    
    
      <category term="机器学习" scheme="https://cunsy192.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="matlab" scheme="https://cunsy192.github.io/tags/matlab/"/>
    
  </entry>
  
  <entry>
    <title>机器学习模型：线性回归（一元）</title>
    <link href="https://cunsy192.github.io/2020/05/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%EF%BC%9A%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <id>https://cunsy192.github.io/2020/05/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%EF%BC%9A%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</id>
    <published>2020-05-30T15:52:00.000Z</published>
    <updated>2020-09-27T10:28:55.134Z</updated>
    
    <content type="html"><![CDATA[<p>本文将从高度数学与线性代数角度来阐述线性回归算法的可执行性，并给出完整实例(matlab编写)。</p><a id="more"></a><p>这里是声明部分：本文章使用markdown语法编写，因为不懂如何插入公式，故使用英译法表示希腊字母，矩阵元素以中国规定的行列表示法表示。</p><p>Part Ⅰ<br>代价函数</p><p>高中的伟大导师蒋（以下简称jml）曾经讲过初级的误差估计（一元函数），在高等数学中<br>我们将对误差估计做出更加精确化的描述（虽然我高中数学非常菜）。</p><p>假设有一组数据集，分别用x和y表示横纵坐标，那么在坐标轴上可以得到一组离散的坐标点：</p><p><img src="/images/pasted-9.png" alt="upload successful">  </p><pre><code>（图片来自于百度）</code></pre><p> 要拟合这些点，也就是得到所谓得“拟合函数”，我们需要两个参数，这里称作theta1和<br> theta2，也就是jml在高中所说的y=kx+b中的b与k。</p><p> 我们先不考虑如何得到这两个参数，先考虑拟合情况好不好，那么，需要一个函数来判定拟合情况，这个函数在高中有接触过，就是把拟合函数在x处的y’与原本的y作差后平方，接着对每一个算出每一个y’,求和，然后再除以2倍的样本值（至于为什么是2倍的样本值，这是因为之后要讲的梯度下降需要对每一个变量求偏导）</p><p>如果具体到公式，以h表示拟合函数，那么：</p><p><img src="/images/pasted-10.png" alt="upload successful"></p><p>代价函数J为：</p><p><img src="/images/pasted-11.png" alt="upload successful"></p><p>此时要想知道拟合情况，只要看J函数的值与0的距离，离0越近，拟合越好。<br>用matlab表示该函数，即</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">J</span> = <span class="title">Jcost</span><span class="params">(X,y,theta)</span></span></span><br><span class="line">m = <span class="built_in">size</span>(X,<span class="number">1</span>); <span class="comment">%计算样本个数</span></span><br><span class="line">predictions = X*theta; <span class="comment">%计算预测值</span></span><br><span class="line">sqrErrors = (predictions - y).^<span class="number">2</span>; <span class="comment">%计算误差</span></span><br><span class="line">J = <span class="number">1</span>/(<span class="number">2</span>*m) * sum(sqrErrors); <span class="comment">%计算损失</span></span><br></pre></td></tr></table></figure><p>测试，得到的结果正确。</p><p> Part Ⅱ<br> 梯度下降</p><p> 现在我们知道如何量化表示拟合函数的拟合情况，但是重要的问题还是没有解决，即如何求出拟合函数。在吴恩达的课程中提供了两种方法，这两种方法均会介绍，但我们着重讲述第二种方法，即梯度下降法。</p><p> ①超定线性方程组与最小二乘解</p><p> 考虑一个方程组(Ax=b)，它由n个未知数组成，却有大于等于n个的方程，这样的方程组称为超定线性方程组。这种方程组经过数学推导（从略）后可以得到以下的式子：</p><p><img src="/images/pasted-12.png" alt="upload successful"></p><p>x就为Ax=b的最小二乘解。原方程组可能无解，向量x的各项带入方程组，使得与方程组中各方程的解最接近（也就是误差最小），这时的解称为最小二乘解。</p><p>②梯度下降法<br>同济高等数学在偏微分一章讲过了梯度下降法的具体内容，按百度百科所说，<br>“梯度下降是迭代法的一种,可以用于求解最小二乘问题。”</p><p>我们知道，代价函数J拥有它的最小解，这个解不一定能用数学方式表达出来，而数学家想出了一种办法，可以有效地让代价函数收敛到它的极小值（不是最小值，所以梯度下降法具有不可避免的误差），具体如下：</p><p>一、初始化变量</p><p>二、对函数J求一个变量的偏导（这里与之前的1/2m呼应，因为求导后平方项会多出一个2，刚好消去，同时，此步是为了求出在初始化条件下在某一点函数的切线，不过我偏导的几何没有弄清，有说错的地方请指正）</p><p>三、给予J的偏导一个速率α,这是为了让J的偏导能向该方向“迈开步子”，“步子”决定了J的收敛时间与收敛区域（α过小，则收敛时间越长；α越大，则收敛区域越大，甚至可能不收敛）</p><p>四、将对J求偏导的那个变量减去α×J’，得到新的变量（这一步称为对变量的一次迭代）</p><p>五、对其他的变量做上述的变化一到四，当所有的变量均完成后一个循环结束，进入下一个循环</p><p>六、迭代多次后，得到最小二乘（或近似）解</p><p>一元的梯度下降公式如下：</p><p><img src="/images/pasted-13.png" alt="upload successful"></p><pre><code>（来自吴恩达：机器学习，网易公开课）</code></pre><p>PartⅢ<br>线性回归模型<br>现在你已经了解了梯度下降的原理，可以结合线性代数的知识来统一模型并加以应用。我从Kaggle上下载了一个二元数据集（300个数据对），他差不多长这样：</p><p><img src="/images/pasted-14.png" alt="upload successful"></p><p>图中的数据有经过升序排列，与原数据不太一样。</p><p>如何利用已有的模型编程解决这些数据的拟合？经过吴老师的启发，我们把二元参数（即θ1、θ2）看作一个列向量θ=[θ1;θ2]，（注意，θ’表示为θ的转置）把上图中的x与y分离，成为两个单独的列向量，同时将x的左侧扩充一个列向量，成为如下样子：</p><p><img src="/images/pasted-15.png" alt="upload successful"></p><p>这样，当我们算(θ’<em>x’)’-y（化简得x</em>θ-y）时就是算出了残差，带入梯度下降公式同时可以得到（以下部分有经过化简与改良，比较难理解）：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:k,</span><br><span class="line">    A(<span class="built_in">i</span>) = (<span class="number">1</span>/m)*sum((x*theta-y).*x(:,<span class="built_in">i</span>));</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>这里的A表示一个2×1的行向量，k为θ的列数（即为2），m为数据对数（这里为300对），sum()为求和函数，而sum函数中的式子表示：</p><p><img src="/images/pasted-16.png" alt="upload successful"></p><p>x(i)0为x矩阵中第1列第i行的元素（全为1）；<br>x(i)1为x矩阵中第2列第i行的元素（为0 0 0 1 ……）。<br>这样，我们就得到了模型的核心算法，接下来就是迭代了，代码略。</p><p>你可能会发现该模型不仅适合于二元线性回归模型，扩充参数矩阵后仍然适用，但那要等到我下一次更新了。</p><p>最后，放出点的离散图与回归过程与回归结果图：</p><p><img src="/images/pasted-17.png" alt="upload successful"></p><pre><code>（数据集的离散图）</code></pre><p><img src="/images/pasted-19.png" alt="upload successful"></p><pre><code>（迭代次数为1000时得到的线性回归图（方向由水平直线向散点图偏移））</code></pre><p><img src="/images/pasted-20.png" alt="upload successful"></p><pre><code>（最终的线性回归图，线比较小容易忽视）</code></pre><p>附上有关数据与程序下载链接：</p><p> <a href="/download/Jcost.m" ">代价函数</a></p><p> <a href="/download/test.csv" ">数据集</a></p><p> <a href="/download/test.m" ">一元线性回归主函数</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将从高度数学与线性代数角度来阐述线性回归算法的可执行性，并给出完整实例(matlab编写)。&lt;/p&gt;
    
    </summary>
    
    
      <category term="我与AI" scheme="https://cunsy192.github.io/categories/%E6%88%91%E4%B8%8EAI/"/>
    
    
      <category term="机器学习" scheme="https://cunsy192.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="matlab" scheme="https://cunsy192.github.io/tags/matlab/"/>
    
  </entry>
  
  <entry>
    <title>《关于线性回归越算越大这件事》-杂谈</title>
    <link href="https://cunsy192.github.io/2020/05/30/%E3%80%8A%E5%85%B3%E4%BA%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E8%B6%8A%E7%AE%97%E8%B6%8A%E5%A4%A7%E8%BF%99%E4%BB%B6%E4%BA%8B%E3%80%8B/"/>
    <id>https://cunsy192.github.io/2020/05/30/%E3%80%8A%E5%85%B3%E4%BA%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E8%B6%8A%E7%AE%97%E8%B6%8A%E5%A4%A7%E8%BF%99%E4%BB%B6%E4%BA%8B%E3%80%8B/</id>
    <published>2020-05-29T16:23:06.000Z</published>
    <updated>2020-09-26T19:29:59.288Z</updated>
    
    <content type="html"><![CDATA[<p>如果说还有什么比做出了线代测试但是把行列式的等号<br>与矩阵的箭头搞错更令人崩溃的话（希望老师改宽点），</p><a id="more"></a><p>那就是5个小时把线性回归模型的参数调整到一个让计算机都<br>难以相信的10的300次方，然后查了半天不知道哪里有问题<br>（一定是kaggle数据集的问题，确信.jpg）。到现在，我连两个<br>参数的模型都没有搞定，后天还有线代的另一次测试，可能这<br>就是人生⑧。话说还是不要买桂花味的<br>百事可乐，它没有桂花味，而且酸，最好买无糖的树莓味可口可乐，<br>虽然无糖差了点灵魂。好久没有搞编程了，希望C++没事。<br>然后介绍下当代智能院学生的科研现状<br>写matlab代码的现状：盯着屏幕发呆——看教程——我懂了——两行代码——<br>这什么东西——盯着屏幕发呆——嗯，仓鼠该喂了<br>                                     科研真是有趣（doge）<br>贴一段越变越大的代码（迭代了100次，因为2000次电脑快炸了）</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">theta = [<span class="number">0</span>;<span class="number">0</span>];</span><br><span class="line">count = <span class="number">0</span>; </span><br><span class="line">iterations = <span class="number">100</span>;</span><br><span class="line">m = <span class="built_in">size</span>(x1,<span class="number">1</span>);</span><br><span class="line">k = <span class="built_in">size</span>(theta,<span class="number">1</span>);</span><br><span class="line"><span class="keyword">while</span>(count &lt; iterations)</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:k,</span><br><span class="line">        A(<span class="built_in">i</span>) = sum((x1*theta-y).*x1(:,<span class="built_in">i</span>));</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    theta = theta - <span class="number">0.0001</span>*A';</span><br><span class="line">    <span class="built_in">disp</span>(count);</span><br><span class="line">    <span class="comment">%disp(A);</span></span><br><span class="line">    <span class="built_in">disp</span>(theta');</span><br><span class="line">    <span class="comment">%plot(count,theta(1,1));</span></span><br><span class="line">    count = count + <span class="number">1</span>;</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>数据集来自kaggle，不放上去了，比较丢人<br>最后贴一段成功的函数，就是吴恩达所说的代价函数的代码实现</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">J</span> = <span class="title">Jcost</span><span class="params">(X,y,theta)</span></span></span><br><span class="line">m = <span class="built_in">size</span>(X,<span class="number">1</span>); <span class="comment">%计算样本个数</span></span><br><span class="line">predictions = X*theta; <span class="comment">%计算预测值</span></span><br><span class="line">sqrErrors = (predictions - y).^<span class="number">2</span>; <span class="comment">%计算误差</span></span><br><span class="line">J = <span class="number">1</span>/(<span class="number">2</span>*m) * sum(sqrErrors); <span class="comment">%计算损失</span></span><br></pre></td></tr></table></figure><p>（果然还是helloworld适合老子）</p><p>5.31补充：</p><p>原来还是学习速率的老问题（吴老师我错了），但是谁知道速率要这么低？不过出来了，还是很激动的。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;如果说还有什么比做出了线代测试但是把行列式的等号&lt;br&gt;与矩阵的箭头搞错更令人崩溃的话（希望老师改宽点），&lt;/p&gt;
    
    </summary>
    
    
      <category term="杂谈类" scheme="https://cunsy192.github.io/categories/%E6%9D%82%E8%B0%88%E7%B1%BB/"/>
    
    
  </entry>
  
  <entry>
    <title>生命游戏（game of life）——初探二维元胞自动机</title>
    <link href="https://cunsy192.github.io/2020/04/18/game-of-life/"/>
    <id>https://cunsy192.github.io/2020/04/18/game-of-life/</id>
    <published>2020-04-18T09:10:00.000Z</published>
    <updated>2020-09-27T10:23:42.594Z</updated>
    
    <content type="html"><![CDATA[<p>谨以此篇纪念John Conway。<br>今天在B站看了观视频的睡前消息，里面督公说的一个名叫“生命游戏”的自动机十分吸引我，加之他对该自动机进行了成功编译，故我准备试试。</p><a id="more"></a><p>规则很简单，具体请查看文末链接①，这里简单再说（这里的规则和督公所说的有出入，以百度为主）：<br>1、细胞具有两种状态，生和死；<br>2、细胞的“周围”：与它相邻的八个细胞；<br>3、细胞的生死取决于周围细胞的生死，如果周围的活细胞等于2，则该细胞生死情况不变；如果周围的活细胞等于3，则该细胞存活；如果周围的细胞不是以上情况，则该细胞死亡。<br>4、对上述规则进行反复运用，最终形成细胞自动机。<br>确定了规则后，开始运用知识进行编程，首先主要到需要一个大矩阵，在开局声明</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> a[<span class="number">101</span>][<span class="number">101</span>];</span><br></pre></td></tr></table></figure><p>后，对矩阵进行初始化，该段代码略去。<br>之后该对矩阵的大小、初始细胞数量进行设定，这里利用随机数函数设定初始细胞数量，具体随机函数可以查看文末链接②，代码如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">printf</span>(<span class="string">"输入方阵的行（列）：\nN="</span>);</span><br><span class="line"><span class="keyword">int</span> N;</span><br><span class="line"><span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;N);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"输入细胞数：\n"</span>);</span><br><span class="line"><span class="keyword">int</span> n;</span><br><span class="line"><span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;n);</span><br><span class="line">srand(time(<span class="literal">NULL</span>));</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; n;i++)</span><br><span class="line">&#123;</span><br><span class="line">a[rand()%<span class="number">11</span>][rand()%<span class="number">11</span>] = <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果要运行该自动机，并且输出各个时刻的运行状态，则需要死循环语句（不要纠结不能退出了，我好久没打C了QAQ），</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; N;i++) <span class="comment">//生命游戏规则主体 </span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>;j &lt; N;j++)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">int</span> k = a[i][j];</span><br><span class="line"><span class="keyword">int</span> cnt = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">if</span>(a[i<span class="number">-1</span>][j<span class="number">-1</span>] == <span class="number">1</span>) cnt++;</span><br><span class="line"><span class="keyword">if</span>(a[i<span class="number">-1</span>][j] == <span class="number">1</span>) cnt++;</span><br><span class="line"><span class="keyword">if</span>(a[i<span class="number">-1</span>][j+<span class="number">1</span>] == <span class="number">1</span>) cnt++;</span><br><span class="line"><span class="keyword">if</span>(a[i][j<span class="number">-1</span>] == <span class="number">1</span>) cnt++;</span><br><span class="line"><span class="keyword">if</span>(a[i][j+<span class="number">1</span>] == <span class="number">1</span>) cnt++;</span><br><span class="line"><span class="keyword">if</span>(a[i+<span class="number">1</span>][j<span class="number">-1</span>] == <span class="number">1</span>) cnt++;</span><br><span class="line"><span class="keyword">if</span>(a[i+<span class="number">1</span>][j] == <span class="number">1</span>) cnt++;</span><br><span class="line"><span class="keyword">if</span>(a[i+<span class="number">1</span>][j+<span class="number">1</span>] == <span class="number">1</span>) cnt++;</span><br><span class="line"><span class="keyword">if</span>(cnt == <span class="number">2</span>) a[i][j] = k;</span><br><span class="line"><span class="keyword">if</span>(cnt == <span class="number">3</span>) a[i][j] = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">if</span>(cnt &gt; <span class="number">3</span> || cnt &lt; <span class="number">2</span>) a[i][j] = <span class="number">0</span>;</span><br><span class="line">cnt = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">          （不要吐槽为什么有这么多<span class="keyword">if</span>，因为我不会取巧TAT）</span><br></pre></td></tr></table></figure><p>但是！如果只有此规则，那么各个时刻的运行状态只是一闪而过，此时要结合单片机的知识：加入延时函数！经测试，比较好观察的延时函数如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">delay</span><span class="params">()</span></span>; <span class="comment">//声明延迟函数 </span></span><br><span class="line">...</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">delay</span><span class="params">()</span> <span class="comment">//函数体</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">int</span> t = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span>(t &lt; <span class="number">10000000</span>) t++; </span><br><span class="line">t = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接下来考虑输出，运用传统二维数组的输出（for二层嵌套），考虑之前输入的矩阵大小输出，代码略。<br>最后运行，但是发现矩阵是一层一层叠下去的，不是一帧一帧闪过去的，于是百度，发现了清屏函数：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">system(<span class="string">"cls"</span>); <span class="comment">//该函数的头文件为stdlib.h，我还真不知道这个神奇的东西</span></span><br></pre></td></tr></table></figure><p>编译并运行！<br>输出结果如下（取截屏，暂时不会录像，在考虑上传B站）：<br><img src="/images/pasted-7.png" alt="upload successful"><br>写在最后：该理论的提出是John Conway，但是他在不久前去世了，做一个小程序，就当对大佬的缅怀吧。</p><p>附代码下载：<br><a href="/download/生命游戏.cpp" ">点击下载</a><br><a href="/download/生命游戏-无随机.cpp" ">点击下载</a></p><p>文末链接<br>①：<a href="https://baike.baidu.com/item/%E7%94%9F%E5%91%BD%E6%B8%B8%E6%88%8F/2926434?fr=aladdin" target="_blank" rel="noopener">https://baike.baidu.com/item/%E7%94%9F%E5%91%BD%E6%B8%B8%E6%88%8F/2926434?fr=aladdin</a><br>②：<a href="https://blog.csdn.net/knigh_yun/article/details/80082944" target="_blank" rel="noopener">https://blog.csdn.net/knigh_yun/article/details/80082944</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;谨以此篇纪念John Conway。&lt;br&gt;今天在B站看了观视频的睡前消息，里面督公说的一个名叫“生命游戏”的自动机十分吸引我，加之他对该自动机进行了成功编译，故我准备试试。&lt;/p&gt;
    
    </summary>
    
    
      <category term="项目" scheme="https://cunsy192.github.io/categories/%E9%A1%B9%E7%9B%AE/"/>
    
    
      <category term="C语言" scheme="https://cunsy192.github.io/tags/C%E8%AF%AD%E8%A8%80/"/>
    
  </entry>
  
  <entry>
    <title>admin插件图片测试</title>
    <link href="https://cunsy192.github.io/2020/04/18/admin%E6%8F%92%E4%BB%B6%E5%9B%BE%E7%89%87%E6%B5%8B%E8%AF%95/"/>
    <id>https://cunsy192.github.io/2020/04/18/admin%E6%8F%92%E4%BB%B6%E5%9B%BE%E7%89%87%E6%B5%8B%E8%AF%95/</id>
    <published>2020-04-18T03:47:00.000Z</published>
    <updated>2020-04-18T03:55:21.133Z</updated>
    
    <content type="html"><![CDATA[<p>以下是测试图片：</p><p><img src="/images/pasted-6.png" alt="upload successful"></p><p>测试结果：成功</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;以下是测试图片：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/pasted-6.png&quot; alt=&quot;upload successful&quot;&gt;&lt;/p&gt;
&lt;p&gt;测试结果：成功&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="测试用内容" scheme="https://cunsy192.github.io/categories/%E6%B5%8B%E8%AF%95%E7%94%A8%E5%86%85%E5%AE%B9/"/>
    
    
      <category term="测试" scheme="https://cunsy192.github.io/tags/%E6%B5%8B%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>翻译问题的解决方案</title>
    <link href="https://cunsy192.github.io/2020/04/18/language/"/>
    <id>https://cunsy192.github.io/2020/04/18/language/</id>
    <published>2020-04-17T16:58:25.000Z</published>
    <updated>2020-04-17T17:06:43.754Z</updated>
    
    <content type="html"><![CDATA[<p>下了next主题后设置了侧边栏，却发现侧边栏的分类是英文，遂百度，有人认为是next自带翻译的问题，<br>但改完依旧，后发现将根目录下_config.yml中的language：后的字符改成next自带的翻译文件zh-Hans，<br>遂解决。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;下了next主题后设置了侧边栏，却发现侧边栏的分类是英文，遂百度，有人认为是next自带翻译的问题，&lt;br&gt;但改完依旧，后发现将根目录下_config.yml中的language：后的字符改成next自带的翻译文件zh-Hans，&lt;br&gt;遂解决。&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="解决方案" scheme="https://cunsy192.github.io/categories/%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"/>
    
    
      <category term="翻译" scheme="https://cunsy192.github.io/tags/%E7%BF%BB%E8%AF%91/"/>
    
  </entry>
  
  <entry>
    <title>16号-杂谈</title>
    <link href="https://cunsy192.github.io/2020/04/16/16%E5%8F%B7-%E6%9D%82%E8%B0%88/"/>
    <id>https://cunsy192.github.io/2020/04/16/16%E5%8F%B7-%E6%9D%82%E8%B0%88/</id>
    <published>2020-04-16T14:13:00.000Z</published>
    <updated>2020-04-16T15:01:51.968Z</updated>
    
    <content type="html"><![CDATA[<pre><code>2020年的4月16日，这一天在我看来是一个具有纪念意义的日子。一切的起源其实要从这天早上两点多我的胡思乱想开始，那时我刚要睡觉，却突发奇想：为什么不做一个博客，然后把它的链接发到微博里去呢？然后我就爬起来了，在搞了两个多小时的疲倦、对明天离散数学和大物的恐惧以及实现了发微博愿望的激动中匆匆睡去。然后，在这天的傍晚，我决定还是把网站做大一些，让他“像一个网站”，于是有了评论和背景。虽然不是很完善，甚至界面简洁地让人分不清哪篇文章是哪篇文章，但是对于今天已经足够了。我还是生活在作业没做完和考试不会做之间。说起来，离散和裂开很像，但我们不会说”裂开数学“，也不会说“我离散了”。但是我真的有点“离散”，在老师的纵容下，几乎没有动大物作业（老师dbq）。还是去写作业吧。对了，如果你有兴趣，一定要玩下泰拉瑞亚，他马上迎来了最后一次更新。</code></pre><p><img src="/2020/04/16/16%E5%8F%B7-%E6%9D%82%E8%B0%88/tr.jpg" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;pre&gt;&lt;code&gt;2020年的4月16日，
这一天在我看来是一个具有纪念意义的日子。
一切的起源其实要从这天早上两点多我的胡思乱想开始，那时我刚要睡觉，却突发奇想：
为什么不做一个博客，然后把它的链接发到微博里去呢？
然后我就爬起来了，在搞了两个多小时的疲倦、对明天离散数学和
      
    
    </summary>
    
    
      <category term="杂谈类" scheme="https://cunsy192.github.io/categories/%E6%9D%82%E8%B0%88%E7%B1%BB/"/>
    
    
  </entry>
  
</feed>
